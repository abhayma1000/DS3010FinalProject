{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.gmail.com\"  # For Gmail, change if using Outlook or another service\n",
    "SMTP_PORT = 465  # SSL port (or use 587 for TLS)\n",
    "SENDER_EMAIL = \"abhaymathur1000@gmail.com\"  # Replace with your email\n",
    "SENDER_PASSWORD = \"blom axpb quot zcdv\"  # Replace with your email password\n",
    "RECEIVER_EMAIL = \"abhaymathur1000@gmail.com\"  # Your email (or another recipient)\n",
    "\n",
    "def send_email(subject, body=\"\"):\n",
    "    \"\"\"Sends an email notification\"\"\"\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = SENDER_EMAIL\n",
    "    msg[\"To\"] = RECEIVER_EMAIL\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
    "            server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Notify when the script completes\n",
    "# send_email(\"âœ… Script Completed\", \"Your Python script has finished running.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:53:21.922358: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 10:53:22.147314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740758002.235064  530487 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740758002.258670  530487 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 10:53:22.454093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/abhay/Documents/Repo/abhayma1000/DS3010FinalProject/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "#NLP Packages\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pyarrow\n",
    "import fastparquet\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/abhay/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4\n",
      "\n",
      "Dataset Load Times:\n",
      "\n",
      "Business Load Time: 2.0878 seconds\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"/kaggle\"):\n",
    "    path = \"/kaggle/input/yelp-dataset\"\n",
    "elif os.path.exists('/home/abhaydesktop/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4'):\n",
    "    path = '/home/abhaydesktop/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4'\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"yelp-dataset/yelp-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path, filename, chunk_size=10000):\n",
    "    chunks = pd.read_json(os.path.join(path, filename), lines=True, chunksize=chunk_size)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        df_list.append(chunk)\n",
    "\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nDataset Load Times:\\n\")\n",
    "start_time = time.time()\n",
    "businesses_df = load_data(path, \"yelp_academic_dataset_business.json\")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Business Load Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "# start_time = time.time()\n",
    "# tips_df = load_data(path, \"yelp_academic_dataset_tip.json\")\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Tips Load Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "reviews_df = load_data(path, \"yelp_academic_dataset_review.json\")\n",
    "# num_rev_load = 2000000\n",
    "num_rev_load = 100000000\n",
    "if num_rev_load < len(reviews_df):\n",
    "    reviews_df = reviews_df.sample(n=num_rev_load)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Review Load Time: {elapsed_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dask = dd.read_parquet(os.path.join(\"data/lemm_stop_reviews_parquet\", \"*.parquet\"), engine=\"pyarrow\", npartitions=20)\n",
    "\n",
    "df = df_dask.partitions[0].compute()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews = df.groupby('business_id')['text'].apply(lambda x: ' '.join(x.sample(min(len(x), 20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews_businesses_df = pd.merge(businesses_df, sampled_reviews, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "last_review_dates = df.groupby('business_id')['date'].max().reset_index()\n",
    "\n",
    "businesses_before_2020 = last_review_dates[last_review_dates['date'] < '2020-01-01']\n",
    "\n",
    "sampled_reviews_businesses_not_after_2020 = pd.merge(sampled_reviews_businesses_df, businesses_before_2020, on='business_id', how='inner')\n",
    "sampled_reviews_businesses_after_2020 = sampled_reviews_businesses_df[~sampled_reviews_businesses_df['business_id'].isin(businesses_before_2020['business_id'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_useful_features(df):\n",
    "    useful_features = df[['latitude', 'longitude', 'stars', 'review_count', 'text']]\n",
    "    return useful_features\n",
    "\n",
    "sampled_reviews_businesses_not_after_2020 = return_useful_features(sampled_reviews_businesses_not_after_2020)\n",
    "sampled_reviews_businesses_after_2020 = return_useful_features(sampled_reviews_businesses_after_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_530487/3787697550.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled_reviews_businesses_not_after_2020['output'] = 0\n",
      "/tmp/ipykernel_530487/3787697550.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled_reviews_businesses_after_2020['output'] = 1\n"
     ]
    }
   ],
   "source": [
    "sampled_reviews_businesses_not_after_2020['output'] = 0\n",
    "sampled_reviews_businesses_after_2020['output'] = 1\n",
    "\n",
    "final_dataset = pd.concat([sampled_reviews_businesses_not_after_2020, sampled_reviews_businesses_after_2020], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.955505</td>\n",
       "      <td>-75.155564</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80</td>\n",
       "      <td>nice pastry shop go bubble tea dinner wong won...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.916116</td>\n",
       "      <td>-82.760461</td>\n",
       "      <td>4.5</td>\n",
       "      <td>100</td>\n",
       "      <td>good sandwich anywhere hand price great genero...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.953949</td>\n",
       "      <td>-75.143226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245</td>\n",
       "      <td>wow great dining adventure huge roll good sake...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.943223</td>\n",
       "      <td>-75.162568</td>\n",
       "      <td>4.5</td>\n",
       "      <td>205</td>\n",
       "      <td>really cute place run husband wife team bibimb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53.546045</td>\n",
       "      <td>-113.499169</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40</td>\n",
       "      <td>really love make wish live close could stop ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>39.960234</td>\n",
       "      <td>-75.196636</td>\n",
       "      <td>3.5</td>\n",
       "      <td>58</td>\n",
       "      <td>order last night take hour come expect we orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>29.941201</td>\n",
       "      <td>-90.128294</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80</td>\n",
       "      <td>cookies cookie cake delicious delivery service...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3930</th>\n",
       "      <td>27.836462</td>\n",
       "      <td>-82.638258</td>\n",
       "      <td>4.5</td>\n",
       "      <td>265</td>\n",
       "      <td>bagel good remind one ny try bagel lox disappo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>39.976974</td>\n",
       "      <td>-75.124114</td>\n",
       "      <td>3.5</td>\n",
       "      <td>93</td>\n",
       "      <td>green egg probably favorite location yet gorge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>27.967029</td>\n",
       "      <td>-82.826794</td>\n",
       "      <td>3.5</td>\n",
       "      <td>32</td>\n",
       "      <td>pleasant place hilton resort beach friendly pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3933 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       latitude   longitude  stars  review_count  \\\n",
       "0     39.955505  -75.155564    4.0            80   \n",
       "1     27.916116  -82.760461    4.5           100   \n",
       "2     39.953949  -75.143226    4.0           245   \n",
       "3     39.943223  -75.162568    4.5           205   \n",
       "4     53.546045 -113.499169    4.0            40   \n",
       "...         ...         ...    ...           ...   \n",
       "3928  39.960234  -75.196636    3.5            58   \n",
       "3929  29.941201  -90.128294    3.0            80   \n",
       "3930  27.836462  -82.638258    4.5           265   \n",
       "3931  39.976974  -75.124114    3.5            93   \n",
       "3932  27.967029  -82.826794    3.5            32   \n",
       "\n",
       "                                                   text  output  \n",
       "0     nice pastry shop go bubble tea dinner wong won...       0  \n",
       "1     good sandwich anywhere hand price great genero...       0  \n",
       "2     wow great dining adventure huge roll good sake...       0  \n",
       "3     really cute place run husband wife team bibimb...       0  \n",
       "4     really love make wish live close could stop ev...       0  \n",
       "...                                                 ...     ...  \n",
       "3928  order last night take hour come expect we orde...       1  \n",
       "3929  cookies cookie cake delicious delivery service...       1  \n",
       "3930  bagel good remind one ny try bagel lox disappo...       1  \n",
       "3931  green egg probably favorite location yet gorge...       1  \n",
       "3932  pleasant place hilton resort beach friendly pe...       1  \n",
       "\n",
       "[3933 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out Data that Belongs to Businesses Under a Certain Threshold\n",
    "minimum_business_reviews = 30\n",
    "reviews_df = reviews_df[reviews_df['business_id'].map(reviews_df['business_id'].value_counts()).gt(minimum_business_reviews)]\n",
    "\n",
    "#Create Merged DataFrame of Remaining Reviewed Businesses, and Split Categories into List\n",
    "df_rb = pd.merge(reviews_df, businesses_df, on=\"business_id\")\n",
    "df_rb['categories'] = df_rb['categories'].str.split(\", \")\n",
    "\n",
    "#Print Metrics\n",
    "print(f\"\\nNumber of Total Businesses Represented: {len(df_rb['business_id'].unique())}\")\n",
    "print(f\"Number of Total Reviews: {len(df_rb)}\")\n",
    "print(f\"Percentage of Reviews Kept: {len(df_rb)/num_rev_load*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note still using old manually loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data\n",
    "\n",
    "Answer the questions: \n",
    "\n",
    "* What is the distribution of reviews over time\n",
    "* What is the range of reviews for different businesses over time\n",
    "* How does covid closures and can we kick those out\n",
    "* Do we predict sentiment separately or do we have one big model? Now, looking more at predicting sentiment. Then sentiment + other things for closure. But this is still on the table\n",
    "* What output do we want for sentiment analysis? Ordinal classification? Specific words?\n",
    "* How would we output sentiment for different years? How do we equalize different amount of years data\n",
    "* How to classify closure. When the last review happened, when a certain threshold of reviews happened. What about renovations, etc. how do we determine the closure\n",
    "* Do research on how to deal with time data, what models we can use, etc.\n",
    "* Explore location data. See if a lot of places are close to each other, what is avg distance, etc. Clustering analysis for points, heatmap. Maybe see what location data is relevant in closures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = reviews_df[\"date\"].dt.year.value_counts().sort_index()\n",
    "plt.bar(year_count.index, year_count.values)\n",
    "plt.title(\"Number of Reviews by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert review_date to datetime\n",
    "df_rb['date'] = pd.to_datetime(df_rb['date'])\n",
    "\n",
    "# Calculate the difference in time between the first and last review for each business\n",
    "time_diffs = df_rb.groupby('business_id')['date'].agg(['min', 'max'])\n",
    "time_diffs['time_diff'] = (time_diffs['max'] - time_diffs['min']).dt.days\n",
    "time_diffs['time_diff_years'] = time_diffs['time_diff'] / 30 / 12\n",
    "\n",
    "# Create a bar plot showing the frequency of time differences\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(time_diffs['time_diff_years'], bins=10, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Time Difference (Years)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Time Differences Between First and Last Review')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should we do one model or two model?\n",
    "\n",
    "Two step model:\n",
    "Pros:\n",
    "* More interperetable\n",
    "* Easier to debug\n",
    "Cons:\n",
    "* Loss of information, nuances\n",
    "* Introduce bias/error in sentiment analysis part\n",
    "\n",
    "Direct end-to-end:\n",
    "Pros:\n",
    "* Avoids cons of two step model\n",
    "* Direct relationship\n",
    "Cons:\n",
    "* More training data to learn complex associations\n",
    "* Less interpretability compared to two step model\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "* I think we should go with end-to-end. We have the training data\n",
    "* We can use Bert to deal with text data, then combine that with a standard MLP for the other data\n",
    "* Combine the outputs in another MLP for the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What output/input do we want for end-to-end model?\n",
    "\n",
    "Input: Reviews with a timestamp. Perhaps we have to summarize reviews or somehow reduce many reviews into a core few to stay within the token limit\n",
    "\n",
    "Output: Ordinal classification of \"In how many years (or months) will this place close\". Not close being infinity\n",
    "\n",
    "\n",
    "# Making the dataset\n",
    "\n",
    "Would have to make a dataset of businesses that closed and one that is still open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we classify closure?\n",
    "\n",
    "Is it when a last review has been made, when reviews trickle down, etc.\n",
    "\n",
    "It seems like the distribution of reviews is dependent sometimes normal/gaussian, but sometimes sporadic\n",
    "\n",
    "There is a good amount of businesses that end before {cutoff_year} and after it so training data would be decent\n",
    "\n",
    "I propose instead of predicting how far in the future a business will close, we just predict whether it will close after {cutoff_year}. 2020 is a good year to do it because right before COVID, and 17% close\n",
    "\n",
    "# Proposed problem\n",
    "\n",
    "Given a random sample of {X} reviews up to 6 months before {cutoff_date}, will the business survive continue to have reviews after the cutoff_date\n",
    "\n",
    "# Data Using\n",
    "\n",
    "* Use each review, but don't include the time of the review\n",
    "    * If it sees many old reviews, it will think the business closed\n",
    "* Use other statistics like location, proximity to other businesses, etc.\n",
    "* Use derived stats like time between reviews, etc. number of reviews, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of either most or least reviewed businesses over time\n",
    "\n",
    "def plot_dist_of_reviews(business_id):\n",
    "    business_name = businesses_df[businesses_df['business_id'] == business_id]['name'].values[0]\n",
    "\n",
    "    business_reviews = df_rb[df_rb['business_id'] == business_id]\n",
    "    business_reviews['date'] = pd.to_datetime(business_reviews['date'])\n",
    "    business_reviews = business_reviews.sort_values('date')\n",
    "    total_business_reviews = business_reviews.shape[0]\n",
    "\n",
    "    plt.hist(business_reviews['date'], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Review Frequency for {business_name}. Total ratings: {total_business_reviews}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "top_businesses = df_rb.groupby('business_id').size().sort_values(ascending=False).head(10)\n",
    "\n",
    "for i in range(10):\n",
    "    plot_dist_of_reviews(top_businesses.index[i])\n",
    "\n",
    "bottom_businesses = df_rb.groupby('business_id').size().sort_values(ascending=False).tail(10)\n",
    "\n",
    "for i in range(10):\n",
    "    plot_dist_of_reviews(bottom_businesses.index[i])\n",
    "\n",
    "\n",
    "average_size = int(df_rb.groupby('business_id').size().sort_values(ascending=False).mean())\n",
    "average_business_id = df_rb.groupby('business_id').size().sort_values(ascending=False).index[average_size]\n",
    "plot_dist_of_reviews(average_business_id)\n",
    "\n",
    "# plot_dist_of_reviews(top_business_id)\n",
    "# plot_dist_of_reviews(bottom_business_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze data\n",
    "\n",
    "Answer the questions: \n",
    "\n",
    "* What is the distribution of reviews over time\n",
    "* What is the range of reviews for different businesses over time\n",
    "* How does covid closures and can we kick those out\n",
    "* Do we predict sentiment separately or do we have one big model? Now, looking more at predicting sentiment. Then sentiment + other things for closure. But this is still on the table\n",
    "* What output do we want for sentiment analysis? Ordinal classification? Specific words?\n",
    "* How would we output sentiment for different years? How do we equalize different amount of years data\n",
    "* How to classify closure. When the last review happened, when a certain threshold of reviews happened. What about renovations, etc. how do we determine the closure\n",
    "* Do research on how to deal with time data, what models we can use, etc.\n",
    "* Explore location data. See if a lot of places are close to each other, what is avg distance, etc. Clustering analysis for points, heatmap. Maybe see what location data is relevant in closures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_year = 2020\n",
    "\n",
    "\n",
    "df_rb['date'] = pd.to_datetime(df_rb['date'])\n",
    "\n",
    "businesses_with_reviews_after_year = df_rb[df_rb['date'] >= f'{cutoff_year}-01-01']['business_id'].unique()\n",
    "\n",
    "\n",
    "reviews_with_no_reviews_after_year = df_rb[~df_rb['business_id'].isin(businesses_with_reviews_after_year)]\n",
    "\n",
    "businesses_with_no_reviews_after_year = reviews_with_no_reviews_after_year['business_id'].unique()\n",
    "\n",
    "reviews_with_no_reviews_after_year\n",
    "\n",
    "print(f\"Number of businesses with reviews after {cutoff_year}:\", len(businesses_with_reviews_after_year))\n",
    "print(f\"Number of businesses with no reviews after {cutoff_year}:\", len(businesses_with_no_reviews_after_year))\n",
    "print(\"Total number of businesses:\", len(df_rb['business_id'].unique()))\n",
    "print(\"Ratio of businesses that close: \", len(businesses_with_no_reviews_after_year) / len(df_rb['business_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random_business_id = random.choice(reviews_with_no_reviews_after_year['business_id'].unique())\n",
    "\n",
    "    plot_dist_of_reviews(random_business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
