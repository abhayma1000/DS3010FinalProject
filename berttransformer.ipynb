{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 13:19:17.936956: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740518358.233836   33558 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740518358.291418   33558 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 13:19:18.976701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/abhaydesktop/Documents/Repo/abhayma1000/DS3010FinalProject/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "#NLP Packages\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in and organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define a generator to read the parquet file in chunks\n",
    "# def read_parquet_in_chunks(file_path, chunksize=10000):\n",
    "#     for chunk in pd.read_parquet(file_path, engine='pyarrow', columns=None):\n",
    "#         yield chunk\n",
    "\n",
    "# # Initialize an empty dataframe\n",
    "# df_rb = pd.DataFrame()\n",
    "\n",
    "# # Read the parquet file in chunks and concatenate them\n",
    "# file_path = os.path.join('data', 'reviews.parquet')\n",
    "# for chunk in read_parquet_in_chunks(file_path):\n",
    "#     df_rb = pd.concat([df_rb, chunk], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from folder:  data/reviews_parquets_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qDVD1-9K8PYkYQ_NjhDfbg</td>\n",
       "      <td>_DClBUD4baUoqrMv_9mC4A</td>\n",
       "      <td>U3grYFIeu6RgAAQgdriHww</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>good need bother loud noise bar restaurant goo...</td>\n",
       "      <td>2018-03-19 15:46:11</td>\n",
       "      <td>Brophy Bros - Santa Barbara</td>\n",
       "      <td>...</td>\n",
       "      <td>CA</td>\n",
       "      <td>93109</td>\n",
       "      <td>34.403759</td>\n",
       "      <td>-119.693992</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2940</td>\n",
       "      <td>1</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>[Cocktail Bars, Nightlife, Seafood, Restaurant...</td>\n",
       "      <td>{'Friday': '11:0-16:0', 'Monday': '0:0-0:0', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ei6N302RZvPLERM6rmIKrw</td>\n",
       "      <td>H2Hri23uglssz0_YdoXwig</td>\n",
       "      <td>3JpJ3b8r5jMdAb1yPmchrQ</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fry chicken course go greasy also well hot chi...</td>\n",
       "      <td>2014-09-15 04:04:07</td>\n",
       "      <td>Prince's Hot Chicken Shack</td>\n",
       "      <td>...</td>\n",
       "      <td>TN</td>\n",
       "      <td>37207</td>\n",
       "      <td>36.230037</td>\n",
       "      <td>-86.761003</td>\n",
       "      <td>4.0</td>\n",
       "      <td>907</td>\n",
       "      <td>0</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>[Restaurants, American (Traditional), Southern]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glPRhDcijNDdBXuYt3o8YQ</td>\n",
       "      <td>P-o2T0qiHCLnza4kx5bsGw</td>\n",
       "      <td>QcMIUKZr9s6LFy7jsDvNVw</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>amazing brunch sunday brandywine prime breakfa...</td>\n",
       "      <td>2017-02-20 14:47:48</td>\n",
       "      <td>Brandywine Prime</td>\n",
       "      <td>...</td>\n",
       "      <td>PA</td>\n",
       "      <td>19317</td>\n",
       "      <td>39.872561</td>\n",
       "      <td>-75.590590</td>\n",
       "      <td>4.0</td>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>[Nightlife, Bars, Steakhouses, Burgers, Restau...</td>\n",
       "      <td>{'Friday': '16:30-21:30', 'Monday': None, 'Sat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_8HrThlckhRGV2lombOL4Q</td>\n",
       "      <td>guIb53iyjk62zx05cmemTw</td>\n",
       "      <td>Bj7DL3SwmYHZYqPduAsa2w</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>go lunch bomb order alligator sausage po boy f...</td>\n",
       "      <td>2018-12-30 01:24:33</td>\n",
       "      <td>Johnny's Po-Boys</td>\n",
       "      <td>...</td>\n",
       "      <td>LA</td>\n",
       "      <td>70130</td>\n",
       "      <td>29.955405</td>\n",
       "      <td>-90.064477</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1451</td>\n",
       "      <td>1</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>[Breakfast &amp; Brunch, Restaurants, Sandwiches]</td>\n",
       "      <td>{'Friday': '8:0-16:30', 'Monday': '8:0-16:30',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g8Y3nnLDyDxWcJ6IYyHvew</td>\n",
       "      <td>rmo1L9DoDc_BRkzsS7M4Ug</td>\n",
       "      <td>KFFoKuUnGry4GyV2qiRwKQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>food okay item hot staff great wait time ridic...</td>\n",
       "      <td>2017-04-02 04:30:26</td>\n",
       "      <td>Riverview Restaurant &amp; Bar</td>\n",
       "      <td>...</td>\n",
       "      <td>NJ</td>\n",
       "      <td>08016</td>\n",
       "      <td>40.080370</td>\n",
       "      <td>-74.858910</td>\n",
       "      <td>3.5</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>{'AcceptsInsurance': None, 'AgesAllowed': None...</td>\n",
       "      <td>[Nightlife, Restaurants, Bars, American (New),...</td>\n",
       "      <td>{'Friday': '15:0-22:0', 'Monday': None, 'Satur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  qDVD1-9K8PYkYQ_NjhDfbg  _DClBUD4baUoqrMv_9mC4A  U3grYFIeu6RgAAQgdriHww   \n",
       "1  ei6N302RZvPLERM6rmIKrw  H2Hri23uglssz0_YdoXwig  3JpJ3b8r5jMdAb1yPmchrQ   \n",
       "2  glPRhDcijNDdBXuYt3o8YQ  P-o2T0qiHCLnza4kx5bsGw  QcMIUKZr9s6LFy7jsDvNVw   \n",
       "3  _8HrThlckhRGV2lombOL4Q  guIb53iyjk62zx05cmemTw  Bj7DL3SwmYHZYqPduAsa2w   \n",
       "4  g8Y3nnLDyDxWcJ6IYyHvew  rmo1L9DoDc_BRkzsS7M4Ug  KFFoKuUnGry4GyV2qiRwKQ   \n",
       "\n",
       "   stars_x  useful  funny  cool  \\\n",
       "0        4       0      0     0   \n",
       "1        5       0      0     0   \n",
       "2        5       0      0     0   \n",
       "3        5       1      0     0   \n",
       "4        2       2      0     0   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  good need bother loud noise bar restaurant goo... 2018-03-19 15:46:11   \n",
       "1  fry chicken course go greasy also well hot chi... 2014-09-15 04:04:07   \n",
       "2  amazing brunch sunday brandywine prime breakfa... 2017-02-20 14:47:48   \n",
       "3  go lunch bomb order alligator sausage po boy f... 2018-12-30 01:24:33   \n",
       "4  food okay item hot staff great wait time ridic... 2017-04-02 04:30:26   \n",
       "\n",
       "                          name  ... state postal_code   latitude   longitude  \\\n",
       "0  Brophy Bros - Santa Barbara  ...    CA       93109  34.403759 -119.693992   \n",
       "1   Prince's Hot Chicken Shack  ...    TN       37207  36.230037  -86.761003   \n",
       "2             Brandywine Prime  ...    PA       19317  39.872561  -75.590590   \n",
       "3             Johnny's Po-Boys  ...    LA       70130  29.955405  -90.064477   \n",
       "4   Riverview Restaurant & Bar  ...    NJ       08016  40.080370  -74.858910   \n",
       "\n",
       "   stars_y  review_count  is_open  \\\n",
       "0      4.0          2940        1   \n",
       "1      4.0           907        0   \n",
       "2      4.0           195        1   \n",
       "3      3.5          1451        1   \n",
       "4      3.5           111        1   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "1  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "2  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "3  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "4  {'AcceptsInsurance': None, 'AgesAllowed': None...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [Cocktail Bars, Nightlife, Seafood, Restaurant...   \n",
       "1    [Restaurants, American (Traditional), Southern]   \n",
       "2  [Nightlife, Bars, Steakhouses, Burgers, Restau...   \n",
       "3      [Breakfast & Brunch, Restaurants, Sandwiches]   \n",
       "4  [Nightlife, Restaurants, Bars, American (New),...   \n",
       "\n",
       "                                               hours  \n",
       "0  {'Friday': '11:0-16:0', 'Monday': '0:0-0:0', '...  \n",
       "1                                               None  \n",
       "2  {'Friday': '16:30-21:30', 'Monday': None, 'Sat...  \n",
       "3  {'Friday': '8:0-16:30', 'Monday': '8:0-16:30',...  \n",
       "4  {'Friday': '15:0-22:0', 'Monday': None, 'Satur...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "save_folder = os.path.join('data', 'reviews_parquets_2')\n",
    "\n",
    "# Load all Parquet files as a Dask DataFrame (lazy loading)\n",
    "print(\"Loading from folder: \", save_folder)\n",
    "df_dask = dd.read_parquet(os.path.join(save_folder, \"*.parquet\"), engine=\"pyarrow\")\n",
    "\n",
    "display(df_dask.npartitions)\n",
    "\n",
    "display(df_dask.head(5))\n",
    "\n",
    "# for partition in df_dask.to_delayed():\n",
    "#     partition_df = partition.compute()  # Load only one partition into memory\n",
    "#     print(partition_df.shape)  # Process chunk-by-chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create inputs (review text) and outputs (categories)\n",
    "inputs = df_dask[\"text\"].compute().values\n",
    "\n",
    "#One-Hot Encode\n",
    "mlb = MultiLabelBinarizer()\n",
    "outputs = mlb.fit_transform(df_dask['categories'].compute()).astype(float)\n",
    "\n",
    "#Total Number of Unique Labels\n",
    "num_labels = outputs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure Tokenizer is Working\n",
    "print('\\nOriginal: {}\\n'.format(inputs[0]))\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: {}\\n'.format(tokenizer.tokenize(inputs[0])))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: {}'.format(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(inputs[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "print('Start Run')\n",
    "start_time = time.time()\n",
    "\n",
    "# For every review...\n",
    "for rev in inputs:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        rev,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        padding = 'max_length',           \n",
    "                        max_length = 200,           # Pad & truncate all sentences. Reviews over 512 so truncate there for now.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation = True        # Truncate at max_length\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Tokenizer Encoding: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "outputs = torch.tensor(outputs).clone().detach()\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', inputs[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, outputs)\n",
    "\n",
    "# Create a 75-15-10 train-validation-test split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size_b4_test = int(len(dataset) - train_size)\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size_b4_test])\n",
    "\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = int(len(val_dataset)-val_size)\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} testing samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = num_labels, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    problem_type = \"multi_label_classification\" # Defaults loss function to BCEWithLogitsLoss\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "\n",
    "# if device == \"cuda:0\":\n",
    "# # Tell pytorch to run this model on the GPU.\n",
    "#     model = model.cuda()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Define Loss function Compatible with Multi-label classification may be redundant givent that \"problem_type\" specified as multi-label-classification in model\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Train with training set #\n",
    "###########################\n",
    "def train(model, iterator, optimizer, criterion, device, scheduler, epoch):\n",
    "    \n",
    "    # Enter Train Mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    #Number of iterations equal to total train dataset / batch size\n",
    "    for step, batch in enumerate(iterator):\n",
    "        #Print progress in epoch\n",
    "        print(f\"Progress: {step+1}/{len(iterator)}\", end='\\r')\n",
    "        # Parse iterator tensor dataset for important information\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        \n",
    "        # Generate prediction\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients and update weights\n",
    "        loss = criterion(output.logits, b_labels) # BCEWithLogitsLoss has sigmoid\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # accumulate train loss\n",
    "        train_loss += loss\n",
    "    \n",
    "    # print completed result\n",
    "    print()\n",
    "    print('Train Loss: %f' % (train_loss))\n",
    "    return train_loss\n",
    "\n",
    "#############################\n",
    "# Validate with testing set #\n",
    "#############################\n",
    "def test(model, iterator, optimizer, criterion, device, epoch):\n",
    "\n",
    "    # Enter Evaluation Mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(iterator):\n",
    "        \n",
    "            #Print progress in epoch\n",
    "            print(f\"Progress: {step+1}/{len(iterator)}\", end='\\r')\n",
    "            print(f'Time per iteration: {datetime.timedelta(seconds=(time.time() - start_time)//(step+1))}', end='\\r')\n",
    "            \n",
    "            # Parse iterator tensor dataset for important information\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # generate prediction\n",
    "            output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "            prob = output.logits.sigmoid()   # BCEWithLogitsLoss has sigmoid\n",
    "            \n",
    "            # record processed data count\n",
    "            total += (b_labels.size(0)*b_labels.size(1))\n",
    "\n",
    "            # take the index of the highest prob as prediction output\n",
    "            THRESHOLD = 0.7\n",
    "            prediction = prob.detach().clone()\n",
    "            prediction[prediction > THRESHOLD] = 1\n",
    "            prediction[prediction <= THRESHOLD] = 0\n",
    "            correct += prediction.eq(b_labels).sum().item()\n",
    "\n",
    "        print()\n",
    "    \n",
    "    #print completed result\n",
    "    acc = 100.*correct/total\n",
    "    print('Correct: %i  / Total: %i / Test Accuracy: %f' % (correct, total, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    \n",
    "    print(f\"\\n===== Epoch {e+1}/{epochs} =====\")\n",
    "    \n",
    "    # training\n",
    "    print(\"Training started ...\")\n",
    "    train(model, train_dataloader, optimizer, criterion, device, scheduler, e)\n",
    "\n",
    "    # validation testing\n",
    "    print(\"Testing started ...\")\n",
    "    test(model, validation_dataloader, optimizer, criterion, device, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.gmail.com\"  # For Gmail, change if using Outlook or another service\n",
    "SMTP_PORT = 465  # SSL port (or use 587 for TLS)\n",
    "SENDER_EMAIL = \"abhaymathur1000@gmail.com\"  # Replace with your email\n",
    "SENDER_PASSWORD = \"blom axpb quot zcdv\"  # Use the generated app password\n",
    "RECEIVER_EMAIL = \"abhaymathur1000@gmail.com\"  # Your email (or another recipient)\n",
    "\n",
    "def send_email(subject, body=\"\"):\n",
    "    \"\"\"Sends an email notification\"\"\"\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = SENDER_EMAIL\n",
    "    msg[\"To\"] = RECEIVER_EMAIL\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
    "            server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Notify when the script completes\n",
    "# send_email(\"✅ Script Completed\", \"Your Python script has finished running.\")\n",
    "send_email(\"Training complete\", \"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('models', 'berttrained')\n",
    "\n",
    "count = 0\n",
    "while os.path.exists(save_path + str(count)):\n",
    "    count += 1\n",
    "\n",
    "model.save_pretrained(save_path + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(os.path.join('models', 'berttrained' + str(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m(model, test_dataloader, optimizer, criterion, device, e)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest model accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "acc = test(model, test_dataloader, optimizer, criterion, device, e)\n",
    "print(\"Test model accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will have to adjust this but not sure how to do so yet without the model - save for later\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            \n",
    "            predictions.extend(list(pred_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predictions:\n",
    "    vec = np.zeros((num_labels))\n",
    "    vec[int(i)] = 1\n",
    "    # print(vec.shape)\n",
    "    print(mlb.inverse_transform(vec.reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlb.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
