{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose is to load in the data and generate tensors for the data. This depends on the specified percentage of the dataset we want to output.\n",
    "\n",
    "Also, there should be different sections of the data so that we can see what goes on at every moment. It should create a copy of the data whenever working with it so that we can see the before and after for the pipeline\n",
    "\n",
    "It should be in linear order where each function does something and outputs the data from the end of that. Since we are not reusing variable names, we will have to plug in different variable names at different places if we want to change things\n",
    "\n",
    "Also, operations should be functions then perhaps return a new dataset. If we do it like this, it makes it easier to compare things and run code multiple times\n",
    "\n",
    "Summary:\n",
    "\n",
    "* Sections, functions for different operations\n",
    "* New variables/variable names for things\n",
    "^ util functions like plotting, etc. are reusable\n",
    "* Work linearly where we create the function in a cell, call it on data in the next cell\n",
    "* At the end, should have an output to tensor function and it does that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import constants\n",
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'part2preprocessing'\n",
    "output_folder, num = utils.get_next_output_folder(f'outputs/{filename}_output')\n",
    "# config = constants.Config(filename,\n",
    "#                           output_folder = output_folder,\n",
    "#                           num_iteration=num,\n",
    "#                           frac_reviews_load=0.00001,\n",
    "#                           minimum_business_reviews=1)\n",
    "config = constants.Config(filename,\n",
    "                          output_folder = output_folder,\n",
    "                          num_iteration=num)\n",
    "print(\"Output folder: \", output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer, get_linear_schedule_with_warmup, BertModel\n",
    "from IPython.display import display, clear_output\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "os.system('./venv/bin/python -m spacy download en_core_web_sm')\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    if os.path.exists(\"/kaggle\"):\n",
    "        return \"/kaggle/input/yelp-dataset\"\n",
    "    elif os.path.exists('/home/abhaydesktop/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4'):\n",
    "        return '/home/abhaydesktop/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4'\n",
    "    else:\n",
    "        return kagglehub.dataset_download(\"yelp-dataset/yelp-dataset\")\n",
    "\n",
    "\n",
    "def load_data(path, filename, total_data_num, frac=1.0):\n",
    "    chunk_size = config.load_in_data_chunk_size\n",
    "    rows_to_load = int(total_data_num * frac)\n",
    "    df = pd.read_json(os.path.join(path, filename), lines=True, chunksize=chunk_size)\n",
    "    \n",
    "    data = []\n",
    "    for chunk in df:\n",
    "        data.append(chunk)\n",
    "        if len(data) * chunk_size >= rows_to_load:\n",
    "            break\n",
    "    \n",
    "    df = pd.concat(data)[:rows_to_load]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_path()\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "print(\"\\nDataset Load Times:\\n\")\n",
    "start_time = time.time()\n",
    "businesses_df = load_data(path, \"yelp_academic_dataset_business.json\", config.total_businesses_in_all, frac=config.frac_businesses_load)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Business Load Time: {elapsed_time:.4f} seconds\")\n",
    "print(f\"Loaded in {config.frac_businesses_load * 100}% of businesses\")\n",
    "\n",
    "# start_time = time.time()\n",
    "# tips_df = load_data(path, \"yelp_academic_dataset_tip.json\")\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Tips Load Time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Loading in {config.frac_reviews_load * 100}% of reviews\")\n",
    "start_time = time.time()\n",
    "reviews_df = load_data(path, \"yelp_academic_dataset_review.json\", config.total_reviews_in_all, frac=config.frac_reviews_load)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Review Load Time: {elapsed_time:.4f} seconds\")\n",
    "print(f\"Loaded in {config.frac_reviews_load * 100}% of reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total businesses:\", len(businesses_df))\n",
    "print(\"Total reviews:\", len(reviews_df))\n",
    "\n",
    "print(\"Total different businesses in reviews:\", len(reviews_df['business_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start filtering the data\n",
    "\n",
    "This is in general filtering for the categories and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_min_business_reviews(rev_df: pd.DataFrame, min_reviews: int):\n",
    "    return rev_df[rev_df['business_id'].map(rev_df['business_id'].value_counts()).ge(min_reviews)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_over_min_reviews = filter_min_business_reviews(reviews_df, config.minimum_business_reviews)\n",
    "print(f\"Number of reviews with at least {config.minimum_business_reviews} reviews for that business: {len(reviews_over_min_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_businesses_with_null_categories(bus_df: pd.DataFrame):\n",
    "    return bus_df[bus_df['categories'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total businesses with null in it: \", len(businesses_df))\n",
    "non_null_businesses_df = filter_out_businesses_with_null_categories(businesses_df)\n",
    "print(\"Total businesses with no null in it: \", len(non_null_businesses_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_uncommon_categories(bus_df: pd.DataFrame, num_cats: int):\n",
    "    all_unique_cats = {}\n",
    "\n",
    "    for cats in bus_df['categories']:\n",
    "        if cats is not None:\n",
    "            for cat in cats.split(', '):\n",
    "                if cat is not None and cat != '' and cat != ' ':\n",
    "                    if cat in all_unique_cats:\n",
    "                        all_unique_cats[cat] += 1\n",
    "                    else:\n",
    "                        all_unique_cats[cat] = 1\n",
    "    \n",
    "    sorted_cats = sorted(all_unique_cats.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_cats = [cat for cat, _ in sorted_cats[:num_cats]]\n",
    "    top_cats_with_numbers = {cat: num for cat, num in sorted_cats[:num_cats]}\n",
    "\n",
    "    # Remove the categories from a business if it is not in the top categories\n",
    "    bus_df['categories'] = bus_df['categories'].apply(lambda cats: ', '.join([cat for cat in cats.split(', ') if cat in top_cats] if cats is not None else None))\n",
    "    # bus_df['categories'] = bus_df['categories'].apply(\n",
    "    # lambda cats: ', '.join([cat for cat in cats.split(', ') if cat in top_cats]) if cats is not None else None\n",
    "# )\n",
    "\n",
    "    return bus_df, top_cats_with_numbers, top_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of businesses before filtering out those with less than {config.top_num_cats_keep}:\", len(businesses_df))\n",
    "filtered_out_uncommon_categories_businesses_df, sorted_top_cats, _ = filter_out_uncommon_categories(non_null_businesses_df, config.top_num_cats_keep)\n",
    "print(f\"Number of businesses after filtering out those with less than {config.top_num_cats_keep}:\", len(filtered_out_uncommon_categories_businesses_df))\n",
    "\n",
    "plt.bar(sorted_top_cats.keys(), [i[1] for i in list(sorted_top_cats.items())])\n",
    "plt.xlabel(\"Categories\")\n",
    "title = \"Number of businesses in each category after filtering out uncommon categories\"\n",
    "plt.title(title)\n",
    "plt.xticks(rotation=90)\n",
    "file_name = title.replace(\" \", \"_\") + \".png\"\n",
    "plt.savefig(f'{os.path.join(config.output_folder, file_name)}')\n",
    "plt.show()\n",
    "\n",
    "display(sorted_top_cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 3_000_000\n",
    "\n",
    "def lemmative(text: str):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.,!¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    text = re.sub(r\"\\n\", \"\", text) # Removing new lines\n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    html = re.compile(r'<.*?>')\n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^,' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    return text\n",
    "\n",
    "def get_adj(text):\n",
    "    tokenized = pos_tag(word_tokenize(text), tagset='universal')\n",
    "    # print(tokenized)\n",
    "    adjs = [word for word, pos in tokenized if pos == 'ADJ']\n",
    "    return ' '.join(adjs)\n",
    "\n",
    "def sentiment_summary(text, fraction=0.3):\n",
    "    \"\"\"\n",
    "    Summarizes text while preserving sentiment.\n",
    "    \n",
    "    :param text: The input text to summarize\n",
    "    :param fraction: The percentage of original text to keep (0.0 to 1.0)\n",
    "    :return: A summarized version of the text\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    # Ensure at least 1 sentence is returned\n",
    "    num_sentences = max(1, int(total_sentences * fraction))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = [abs(analyzer.polarity_scores(s)['compound']) for s in sentences]\n",
    "    \n",
    "    # Combine importance & sentiment (weighted sum)\n",
    "    combined_scores = sentence_scores * np.array(sentiment_scores)\n",
    "    \n",
    "    # Pick top sentences\n",
    "    top_sentence_indices = combined_scores.argsort()[-num_sentences:][::-1]\n",
    "    summary = [sentences[i] for i in sorted(top_sentence_indices)]\n",
    "    \n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Length of text (total) before cleaning:\", reviews_over_min_reviews['text'].apply(len).sum())\n",
    "# print(\"Average length of text before cleaning:\", reviews_over_min_reviews['text'].apply(len).mean())\n",
    "\n",
    "# cleaned_reviews = reviews_over_min_reviews.copy()\n",
    "# cleaned_reviews['text'] = cleaned_reviews['text'].apply(lambda x: clean_text(x))\n",
    "# print(\"Length of text (total) after cleaning:\", cleaned_reviews['text'].apply(len).sum())\n",
    "# print(\"Average length of text after cleaning:\", cleaned_reviews['text'].apply(len).mean())\n",
    "\n",
    "# lemmatized_reviews = cleaned_reviews.copy()\n",
    "# lemmatized_reviews['text'] = cleaned_reviews['text'].apply(lambda x: lemmative(x))\n",
    "# print(\"Length of text (total) after lemmatizing:\", lemmatized_reviews['text'].apply(len).sum())\n",
    "# print(\"Average length of text after lemmatizing:\", lemmatized_reviews['text'].apply(len).mean())\n",
    "\n",
    "# adj_reviews = lemmatized_reviews.copy()\n",
    "# adj_reviews['text'] = lemmatized_reviews['text'].apply(lambda x: get_adj(x))\n",
    "# print(\"Length of text (total) after getting adjectives:\", adj_reviews['text'].apply(len).sum())\n",
    "# print(\"Average length of text after getting adjectives:\", adj_reviews['text'].apply(len).mean())\n",
    "\n",
    "# sentiment_summarized_reviews = adj_reviews.copy()\n",
    "# sentiment_summarized_reviews['text'] = adj_reviews['text'].apply(lambda x: sentiment_summary(x))\n",
    "# print(\"Length of text (total) after sentiment summarizing:\", sentiment_summarized_reviews['text'].apply(len).sum())\n",
    "# print(\"Average length of text after sentiment summarizing:\", sentiment_summarized_reviews['text'].apply(len).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To inspect the effect of the transformations\n",
    "\n",
    "# print(\"Taking a random review and applying transformations to see the effect of each transformation\")\n",
    "# random_index = np.random.randint(0, len(lemmatized_reviews))\n",
    "\n",
    "# random_review_over_min = reviews_over_min_reviews['text'].iloc[random_index]\n",
    "# print(\"Review before cleaning:\\n\", random_review_over_min)\n",
    "# print(\"\\n\\n\")\n",
    "\n",
    "# random_review_cleaned = cleaned_reviews['text'].iloc[random_index]\n",
    "# print(\"Review after cleaning:\\n\", random_review_cleaned)\n",
    "# print(\"\\n\\n\")\n",
    "\n",
    "# random_review_lemmatized = lemmatized_reviews['text'].iloc[random_index]\n",
    "# print(\"Review after lemmatizing:\\n\", random_review_lemmatized)\n",
    "# print(\"\\n\\n\")\n",
    "\n",
    "# random_review_adj = adj_reviews['text'].iloc[random_index]\n",
    "# print(\"Review after adjectivization:\\n\", random_review_adj)\n",
    "# print(\"\\n\\n\")\n",
    "\n",
    "# random_review_sentiment_summarized = sentiment_summarized_reviews['text'].iloc[random_index]\n",
    "# print(\"Review after sentiment summarizing:\\n\", random_review_sentiment_summarized)\n",
    "# print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return_guest_percentage(rev_df: pd.DataFrame):\n",
    "    # Count total unique guests per business\n",
    "    total_guests = rev_df.groupby('business_id')['user_id'].nunique().reset_index()\n",
    "    total_guests = total_guests.rename(columns={'user_id': 'total_guest_count'})\n",
    "\n",
    "    # Count the number of reviews per user per business\n",
    "    repeat_visits = rev_df.groupby(['business_id', 'user_id']).size().reset_index(name='review_count')\n",
    "\n",
    "    # Filter businesses where a user has more than one review (return guests)\n",
    "    repeat_visits = repeat_visits[repeat_visits['review_count'] > 1]\n",
    "\n",
    "    # Count return guests per business\n",
    "    return_guests = repeat_visits.groupby('business_id')['user_id'].nunique().reset_index()\n",
    "    return_guests = return_guests .rename(columns={'user_id': 'return_guest_count'})\n",
    "\n",
    "\n",
    "    # Merge return guests count with total guests count\n",
    "    result = total_guests.merge(return_guests, on='business_id', how='left').fillna(0)\n",
    "\n",
    "    # Calculate return guest percentage\n",
    "    result['return_percentage'] = (result['return_guest_count'] / result['total_guest_count']) * 100\n",
    "    result['return_percentage'] = result['return_percentage'].fillna(0)  # Replace NaN with 0%\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Everything that takes in reviews df (like this) needs to be tested later\n",
    "# When using the big, full dataset\n",
    "# And can get rid of two of these because already self-explanatory and in the model already\n",
    "guest_return_guest_businesses_df = calculate_return_guest_percentage(reviews_over_min_reviews)\n",
    "\n",
    "guest_return_guest_businesses_df = pd.merge(filtered_out_uncommon_categories_businesses_df, guest_return_guest_businesses_df, on='business_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_func(x):\n",
    "    # I want from 1 to 10\n",
    "    if x <= 1:\n",
    "        return 0\n",
    "    elif x <= 10 and x > 1:\n",
    "        return utils.transform_range(x, 1, 10, 1, 5)\n",
    "    elif x <= 100 and x > 10:\n",
    "        return utils.transform_range(x, 10, 100, 5, 8)\n",
    "    else:\n",
    "        return utils.transform_range(x, 100, 1000, 8, 10)\n",
    "\n",
    "\n",
    "def get_franchise_score(bus_df: pd.DataFrame):\n",
    "    name_counts = bus_df['name'].value_counts()\n",
    "\n",
    "    bus_df['franchise_score'] = bus_df['name'].apply(lambda x: trans_func(name_counts[x])).fillna(0).astype(float)\n",
    "\n",
    "    return bus_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "franchise_score_bus_df = get_franchise_score(guest_return_guest_businesses_df)\n",
    "\n",
    "# Plotting the distribution of franchise scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(franchise_score_bus_df['franchise_score'], bins=30, kde=True)\n",
    "plt.xlabel('Franchise Score')\n",
    "plt.ylabel('Frequency')\n",
    "title = \"Distribution of Franchise Scores\"\n",
    "plt.title(title)\n",
    "file_name = title.replace(\" \", \"_\") + \".png\"\n",
    "plt.savefig(f'{os.path.join(config.output_folder, file_name)}')\n",
    "plt.show()\n",
    "\n",
    "print(\"While there are majority businesses with one (or even two) locations\", \n",
    "      \"there are a few businesses with many locations. Ex: McDonald's, UPS Store, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "\n",
    "gdf = gpd.read_file(\"inputs/shapefiles/cb_2023_us_state_500k.shp\", SHAPE_RESTORE_SHX=True)\n",
    "\n",
    "gdf = gdf.set_crs('EPSG:4326')\n",
    "\n",
    "usa = gdf.rename(columns={'STUSPS': 'state'})\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(20, 12))\n",
    "gdf.plot(ax=ax, color='white', edgecolor='black')\n",
    "state_counts = franchise_score_bus_df['state'].value_counts()\n",
    "sizes = franchise_score_bus_df['state'].map(state_counts)\n",
    "size_multiplier = 0.01\n",
    "\n",
    "ax.scatter(franchise_score_bus_df['longitude'], franchise_score_bus_df['latitude'], s=sizes * size_multiplier, alpha=0.5)\n",
    "title = \"Plot of the locations of different businesses in the US. Size is proportional to the number of businesses in that state\"\n",
    "plt.title(title)\n",
    "file_name = title.replace(\" \", \"_\") + \".png\"\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "ax.set_xlim([-130, -60])  # Adjust the longitude limits as needed\n",
    "ax.set_ylim([20, 55])\n",
    "plt.savefig(f'{os.path.join(config.output_folder, file_name)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_state(bus_df: pd.DataFrame):\n",
    "    return pd.concat([bus_df, pd.get_dummies(bus_df['state'], prefix='state').astype(int)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_bus_df = get_one_hot_state(franchise_score_bus_df)\n",
    "\n",
    "state_bus_df.iloc[1]\n",
    "\n",
    "# Plotting the frequencies of each state\n",
    "state_counts = state_bus_df['state'].value_counts()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=state_counts.index, y=state_counts.values, palette='viridis')\n",
    "title = 'Frequencies of Each State'\n",
    "plt.title(title)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "file_name = title.replace(\" \", \"_\") + \".png\"\n",
    "plt.savefig(f'{os.path.join(config.output_folder, file_name)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the things with the highest density\n",
    "# Then plot them and see how close in real life\n",
    "# \n",
    "\n",
    "def get_density_score(bus_df: pd.DataFrame):\n",
    "    coords = bus_df[['latitude', 'longitude']].T\n",
    "\n",
    "    kde = gaussian_kde(coords, bw_method=0.1)\n",
    "    bus_df['density_score'] = kde(coords).astype(float)\n",
    "    bus_df['density_score'] = utils.transform_range(bus_df['density_score'], bus_df['density_score'].min(), bus_df['density_score'].max(), 0, 1)\n",
    "\n",
    "    return bus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_score_bus_df = get_density_score(state_bus_df)\n",
    "\n",
    "density_score_bus_df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Describing the density score: \", density_score_bus_df['density_score'].describe())\n",
    "\n",
    "print(\"Plotting the places with the highest density score as red triangles and the low density places in blue circles\")\n",
    "\n",
    "highest_density = density_score_bus_df[density_score_bus_df['density_score'] == density_score_bus_df['density_score'].max()]\n",
    "\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "\n",
    "gdf = gpd.read_file(\"inputs/shapefiles/cb_2023_42_bg_500k.shp\", SHAPE_RESTORE_SHX=True)\n",
    "\n",
    "gdf = gdf.set_crs('EPSG:4326')\n",
    "\n",
    "only_PA_bus_df = density_score_bus_df[density_score_bus_df['state'] == 'PA']\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(20, 12))\n",
    "gdf.plot(ax=ax, color='white', edgecolor='black')\n",
    "sizes = only_PA_bus_df['density_score']\n",
    "size_multiplier = 0.4\n",
    "\n",
    "threshold = 0.99\n",
    "high_density_places = only_PA_bus_df[only_PA_bus_df['density_score'] > threshold]\n",
    "\n",
    "\n",
    "ax.scatter(only_PA_bus_df['longitude'], only_PA_bus_df['latitude'], s=size_multiplier * (sizes ** 2), alpha=1)\n",
    "\n",
    "\n",
    "ax.scatter(high_density_places['longitude'], high_density_places['latitude'], s=75, color='red', marker='^', label='High Density')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "title = \"Plot of the densities of different locations within PA. Size is proportional to density\"\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "ax.set_xlim([-76, -74.5])\n",
    "ax.set_ylim([39.75, 40.5])\n",
    "file_name = title.replace(\" \", \"_\") + \".png\"\n",
    "plt.savefig(f'{os.path.join(config.output_folder, file_name)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into closed vs. not closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of businesses that are closed:\")\n",
    "print(density_score_bus_df['is_open'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bus_open_closed(rev_df: pd.DataFrame, cutoff_date='2020-02-01'):\n",
    "    cutoff_date = pd.to_datetime(cutoff_date)\n",
    "    last_review_dates = rev_df.groupby('business_id')['date'].max().reset_index()\n",
    "    print(\"Last review dates: \", last_review_dates)\n",
    "    last_review_dates['is_open_after_date'] = last_review_dates['date'].apply(lambda x: 1 if x > cutoff_date else 0)\n",
    "    return last_review_dates[['business_id', 'date', 'is_open_after_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_review_dates_bus_df = get_bus_open_closed(reviews_over_min_reviews)\n",
    "\n",
    "is_open_merged_bus_df = density_score_bus_df.merge(last_review_dates_bus_df, on='business_id', how='inner')\n",
    "print(\"Is open merged bus df: \", is_open_merged_bus_df)\n",
    "print(\"Len of last review dates bus df: \", len(last_review_dates_bus_df))\n",
    "print(\"Len of pre-merged bus df: \", len(density_score_bus_df))\n",
    "print(\"Len of merged bus df: \", len(is_open_merged_bus_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with reviews and text data\n",
    "\n",
    "Right now just going to use ```reviews_over_min_reviews```, which is the basic reviews then going to pad.\n",
    "\n",
    "Since the average length is 480, which is close to 512 (as of the small reviews df), then we are just going to keep one review and pad/truncate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train, val, test\n",
    "def split_data(bus_df: pd.DataFrame, \n",
    "               train_frac=config.train_test_val_split[0], \n",
    "               val_frac=config.train_test_val_split[1], \n",
    "               test_frac=config.train_test_val_split[2]):\n",
    "    train, val, test = np.split(bus_df.sample(frac=1), [int(train_frac * len(bus_df)), int((train_frac + val_frac) * len(bus_df))])\n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bus_df, val_bus_df, test_bus_df = split_data(is_open_merged_bus_df)\n",
    "\n",
    "print(\"Length of original bus df: \", len(is_open_merged_bus_df))\n",
    "print(\"Length of train bus df: \", len(train_bus_df))\n",
    "print(\"Length of val bus df: \", len(val_bus_df))\n",
    "print(\"Length of test bus df: \", len(test_bus_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of the day, need to get a merged df of the text and the business data\n",
    "# However I get that text is up to me\n",
    "# Right now, it just takes the raw, random text of a review for that biz\n",
    "# It prob pads or truncates in the model itself.\n",
    "\n",
    "# So I have the other one where it gets one sample for each business.\n",
    "# I want it here where it gets all the reviews for each of the businesses in the train df\n",
    "# Then with all the reviews for each business, it applies the transformations to the text\n",
    "# Then it adds all the texts to a list and breaks them up into 512 character chunks\n",
    "# It adds each chunk to the train df. Max of 4 reviews per business. If there are more, it takes the first 4\n",
    "# If there are less, it lets the tokenizer pad it\n",
    "\n",
    "# This should increase the number of occurances, so check that with old outputs\n",
    "def get_train_df_text(bus_df: pd.DataFrame, rev_df: pd.DataFrame, max_occurances: int):\n",
    "    # First need to group reviews by business and concatenate the text\n",
    "    rev_df_bus_df = rev_df.groupby('business_id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "    # Then merge the bitext with the business df\n",
    "    bus_df_text = pd.merge(bus_df, rev_df_bus_df, on='business_id', how='inner')\n",
    "    print(\"Length of the bus df with the one long concatenated text: \", len(bus_df_text))\n",
    "    # This leaves with a df of the business_id and the big text of all the reviews for that business\n",
    "\n",
    "    # cap the text length to x\n",
    "    bus_df_text['text'] = bus_df_text['text'].apply(lambda x: x[:config.max_text_length])\n",
    "    print(\"Capping the length of all the text to \", config.max_text_length)\n",
    "\n",
    "    # Then apply the transformations to the text\n",
    "    bus_df_text['text'] = bus_df_text['text'].apply(lambda x: clean_text(x))\n",
    "    print(\"Description of length of each text after cleaning: \", bus_df_text['text'].apply(len).describe())\n",
    "    bus_df_text['text'] = bus_df_text['text'].apply(lambda x: lemmative(x))\n",
    "    print(\"Description of length of each text after lemmatizing: \", bus_df_text['text'].apply(len).describe())\n",
    "    # gonna leave adjectives out for now\n",
    "    # bus_df_text['text'] = bus_df_text['text'].apply(lambda x: get_adj(x))\n",
    "\n",
    "    def split_text(text, chunk_size=config.tokenizer_max_length):\n",
    "        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    bus_df_text['text'] = bus_df_text['text'].apply(lambda x: split_text(x))\n",
    "    print(\"Description of number of windows for each business: \", bus_df_text['text'].apply(len).describe())\n",
    "\n",
    "    bus_df_text = bus_df_text.explode('text', ignore_index=True)\n",
    "    bus_df_text['text'] = bus_df_text['text'].apply(lambda x: x.strip())\n",
    "    print(\"Description of length of each text after splitting: \", bus_df_text['text'].apply(len).describe())\n",
    "    print(\"Length of the bus rev df after exploding: \", len(bus_df_text))\n",
    "    print(\"Random element of the bus rev df after exploding: \", bus_df_text.sample(1))\n",
    "\n",
    "    return bus_df_text\n",
    "\n",
    "\n",
    "def get_text_for_each_val_test_business(bus_df: pd.DataFrame, rev_df: pd.DataFrame, num_sample=1):\n",
    "    rev_df_bus_df = rev_df.groupby('business_id').apply(lambda x: x.sample(num_sample)).reset_index(drop=True)\n",
    "    return pd.merge(bus_df, rev_df_bus_df[['business_id', 'text']], on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "full_bus_rev_df = get_text_for_each_val_test_business(is_open_merged_bus_df, reviews_over_min_reviews)\n",
    "utils.send_email(\"About to start the train split thing\", \"Look bro\")\n",
    "# To not run the whole train thing, set it to use the val/test functions\n",
    "train_full_bus_rev_df = get_train_df_text(train_bus_df, reviews_over_min_reviews, max_occurances=10)\n",
    "val_full_bus_rev_df = get_text_for_each_val_test_business(val_bus_df, reviews_over_min_reviews, num_sample=1)\n",
    "test_full_bus_rev_df = get_text_for_each_val_test_business(test_bus_df, reviews_over_min_reviews, num_sample=1)\n",
    "print(\"Giving the test and val datasets only one review per business\")\n",
    "\n",
    "print(\"Length of full bus rev df: \", len(full_bus_rev_df))\n",
    "print(\"Length of density score bus df: \", len(density_score_bus_df))\n",
    "print(\"Length of reviews over min reviews: \", len(reviews_over_min_reviews))\n",
    "print(\"Length of train full bus rev df: \", len(train_full_bus_rev_df))\n",
    "print(\"Because I sample 2 from each business for train, it has double than it should, which is: \", len(train_bus_df))\n",
    "print(\"Length of val full bus rev df: \", len(val_full_bus_rev_df))\n",
    "print(\"Length of test full bus rev df: \", len(test_full_bus_rev_df))\n",
    "print(\"Random sample of full bus rev df: \", full_bus_rev_df.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_useful_features(bus_rev_df: pd.DataFrame):\n",
    "    to_keep_list = ['latitude', 'longitude', 'stars', 'review_count', 'return_percentage', 'franchise_score', 'density_score', 'text', 'is_open_after_date']\n",
    "    for state in bus_rev_df['state'].unique():\n",
    "        to_keep_list.append(f'state_{state}')\n",
    "    print(\"All the columns: \", bus_rev_df.columns)\n",
    "    print(\"Columns to keep: \", to_keep_list)\n",
    "    print(\"Total number of columns to keep: \", len(to_keep_list))\n",
    "\n",
    "    return to_keep_list\n",
    "\n",
    "def only_keep_useful_features(bus_rev_df: pd.DataFrame, to_keep_list: list):\n",
    "    return bus_rev_df[to_keep_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep_list = get_useful_features(full_bus_rev_df)\n",
    "\n",
    "final_bus_rev_df = only_keep_useful_features(full_bus_rev_df, to_keep_list)\n",
    "\n",
    "train_final_bus_rev_df = only_keep_useful_features(train_full_bus_rev_df, to_keep_list) \n",
    "val_final_bus_rev_df = only_keep_useful_features(val_full_bus_rev_df, to_keep_list)\n",
    "test_final_bus_rev_df = only_keep_useful_features(test_full_bus_rev_df, to_keep_list)\n",
    "\n",
    "print(\"After only keeping useful features...\")\n",
    "print(\"Shape of final bus rev df: \", final_bus_rev_df.shape)\n",
    "print(\"Shape of train final bus rev df: \", train_final_bus_rev_df.shape)\n",
    "print(\"Shape of val final bus rev df: \", val_final_bus_rev_df.shape)\n",
    "print(\"Shape of test final bus rev df: \", test_final_bus_rev_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Percentage of the businesses that close/stay open:\\n\", final_bus_rev_df['is_open_after_date'].value_counts(normalize=True) * 100)\n",
    "\n",
    "def equalize_data_on_output(rev_bus_df: pd.DataFrame):\n",
    "    lens = (len(rev_bus_df[rev_bus_df['is_open_after_date'] == 1]), len(rev_bus_df[rev_bus_df['is_open_after_date'] == 0]))\n",
    "    print(\"Lens: \", lens)\n",
    "    min_len = min(lens)\n",
    "    return rev_bus_df.groupby('is_open_after_date').apply(lambda x: x.sample(min_len)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_final_bus_rev_df = equalize_data_on_output(final_bus_rev_df)\n",
    "\n",
    "train_equalized_final_bus_rev_df = equalize_data_on_output(train_final_bus_rev_df)\n",
    "\n",
    "\n",
    "print(\"Len of equalized final bus rev df: \", len(equalized_final_bus_rev_df))\n",
    "print(\"Percentage of the businesses that close/stay open after equalizing:\\n\", equalized_final_bus_rev_df['is_open_after_date'].value_counts(normalize=True) * 100)\n",
    "print(\"Len of train equalized final bus rev df: \", len(train_equalized_final_bus_rev_df))\n",
    "print(\"Percentage of the train businesses that close/stay open after equalizing:\\n\", train_equalized_final_bus_rev_df['is_open_after_date'].value_counts(normalize=True) * 100)\n",
    "print(\"Percentage of the val businesses that close/stay open after equalizing:\\n\", val_final_bus_rev_df['is_open_after_date'].value_counts(normalize=True) * 100)\n",
    "print(\"Percentage of the test businesses that close/stay open after equalizing:\\n\", test_final_bus_rev_df['is_open_after_date'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tensors\n",
    "\n",
    "Tokenize text and create tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensors(final_bus_rev_df: pd.DataFrame, output_var='is_open_after_date'):\n",
    "    all_columns = final_bus_rev_df.columns\n",
    "\n",
    "    text_inputs = final_bus_rev_df['text'].values\n",
    "\n",
    "    nn_input_features = [col for col in all_columns if col != 'text' and col != output_var]\n",
    "\n",
    "    for feature in nn_input_features:\n",
    "        final_bus_rev_df[feature] = pd.to_numeric(final_bus_rev_df[feature], errors='coerce').fillna(0)\n",
    "\n",
    "    print(\"Features for nn: \", nn_input_features)\n",
    "\n",
    "    nn_inputs = torch.tensor(final_bus_rev_df[nn_input_features].values, dtype=torch.float32)\n",
    "\n",
    "    print(\"NN input shape: \", nn_inputs.shape)\n",
    "    print(\"Sample NN input: \", nn_inputs[0])\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    for rev in text_inputs:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            rev,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length=config.tokenizer_max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    outputs = torch.tensor(final_bus_rev_df[output_var].values, dtype=torch.float32)\n",
    "\n",
    "    return input_ids, attention_masks, nn_inputs, outputs\n",
    "\n",
    "def get_data_tensors_with_string(final_bus_rev_df: pd.DataFrame, output_var='is_open_after_date'):\n",
    "    all_columns = final_bus_rev_df.columns\n",
    "\n",
    "    encoded_strings = [torch.tensor(list(s.encode(\"utf-8\"))) for s in final_bus_rev_df['text'].values]\n",
    "    max_len = max(len(t) for t in encoded_strings)\n",
    "    padded_strings = torch.stack([torch.nn.functional.pad(t, (0, max_len - len(t))) for t in encoded_strings])\n",
    "\n",
    "\n",
    "    nn_input_features = [col for col in all_columns if col != 'text' and col != output_var]\n",
    "\n",
    "    for feature in nn_input_features:\n",
    "        final_bus_rev_df[feature] = pd.to_numeric(final_bus_rev_df[feature], errors='coerce').fillna(0)\n",
    "\n",
    "    print(\"Features for nn: \", nn_input_features)\n",
    "\n",
    "    nn_inputs = torch.tensor(final_bus_rev_df[nn_input_features].values, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    outputs = torch.tensor(final_bus_rev_df[output_var].values, dtype=torch.float32)\n",
    "\n",
    "    return padded_strings, nn_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_tensors(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    names = ['train', 'val', 'test']\n",
    "    for i, df in enumerate([train_df, val_df, test_df]):\n",
    "        input_ids, attention_masks, nn_inputs, outputs = get_data_tensors(df)\n",
    "        torch.save({'input_ids': input_ids, 'attention_masks': attention_masks, 'nn_inputs': nn_inputs, 'outputs': outputs}, \n",
    "            os.path.join(config.output_folder, f'all_tensors_percentage_{config.frac_reviews_load}_{names[i]}.pth'))\n",
    "    \n",
    "    padded_strings, nn_inputs, outputs = get_data_tensors_with_string(test_df)\n",
    "    torch.save({'padded_strings': padded_strings, 'nn_inputs': nn_inputs, 'outputs': outputs},\n",
    "               os.path.join(config.output_folder, f'all_tensors_percentage_{config.frac_reviews_load}_test_strings.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_val_test_tensors(train_equalized_final_bus_rev_df, val_final_bus_rev_df, test_final_bus_rev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing sample businesses, their text, and their output variable\")\n",
    "\n",
    "for i in range(5):\n",
    "    bus = test_full_bus_rev_df.sample(1)\n",
    "    print(\"Business name: \", bus['name'].values[0])\n",
    "    print('Business text: ', bus['text'].values[0])\n",
    "    print('Business output variable: ', bus['is_open_after_date'].values[0])\n",
    "    print(\"Business all data: \\n\", bus)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the config's state dict as a file\n",
    "with open(os.path.join(config.output_folder, 'config.json'), 'w') as f:\n",
    "    json.dump(config.__dict__, f, indent=4)\n",
    "\n",
    "utils.send_email(f\"✅ Finished {config.filename} execution\", f\"Took {utils.get_time_from_start(start_time)}\\nConfig: {config.__dict__}\\nAbhayUpdateEmail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Honestly not really used here, but I'm keeping it here for now\n",
    "def save_to_parquet(df, path, percentage_of_total_data=1.0):\n",
    "    new_path = utils.get_next_filename(f'{path}_percentage_{percentage_of_total_data}.parquet')\n",
    "    df.to_parquet(new_path, engine='pyarrow')\n",
    "    print(f\"Saved to {new_path}\")\n",
    "\n",
    "# save_to_parquet(lemmatized_reviews, f\"data/{file_name}\", config.frac_reviews_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
