{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup Environment, Load Data","metadata":{}},{"cell_type":"code","source":"#Import Libraries\nimport os\nimport json\nimport csv\nimport matplotlib.pyplot as plt\nimport sklearn\nimport tensorflow\nimport numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport gc\nimport random\nimport re\nimport torch\nimport torch.nn as nn\nimport time\nimport spacy\n\n#NLP Packages\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Dense\nfrom tensorflow.keras.layers import Flatten, LSTM\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Concatenate\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport transformers\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\nfrom IPython.display import display, clear_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:09:08.312731Z","iopub.execute_input":"2025-02-18T22:09:08.313079Z","iopub.status.idle":"2025-02-18T22:09:08.321441Z","shell.execute_reply.started":"2025-02-18T22:09:08.313050Z","shell.execute_reply":"2025-02-18T22:09:08.320323Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:53:09.313169Z","iopub.execute_input":"2025-02-18T21:53:09.313534Z","iopub.status.idle":"2025-02-18T21:53:09.321028Z","shell.execute_reply.started":"2025-02-18T21:53:09.313508Z","shell.execute_reply":"2025-02-18T21:53:09.319902Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/yelp-dataset/Dataset_User_Agreement.pdf\n/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\n/kaggle/input/yelp-dataset/yelp_academic_dataset_checkin.json\n/kaggle/input/yelp-dataset/yelp_academic_dataset_business.json\n/kaggle/input/yelp-dataset/yelp_academic_dataset_tip.json\n/kaggle/input/yelp-dataset/yelp_academic_dataset_user.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#Load in Data\ndef load_data(path, filename, truncate=None):\n    data_file = open(os.path.join(path, filename))\n    data = []\n    if truncate is not None:\n        for i, line in enumerate(data_file):\n            data.append(json.loads(line))\n            if i == truncate:\n                break\n        # for line in data_file[0:truncate]:\n        # data.append(json.loads(line))\n    else:\n        for line in data_file:\n            data.append(json.loads(line))\n    ret_df = pd.DataFrame(data)\n    data_file.close()\n    return ret_df\n\npath = \"/kaggle/input/yelp-dataset\"\n\nprint(\"\\nDataset Load Times:\\n\")\nstart_time = time.time()\nbusinesses_df = load_data(path, \"yelp_academic_dataset_business.json\")\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Business Load Time: {elapsed_time:.4f} seconds\")\n\nnum_rev_load = 100000\nstart_time = time.time()\nreviews_df = load_data(path, \"yelp_academic_dataset_review.json\", truncate=num_rev_load)\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Review Load Time: {elapsed_time:.4f} seconds\")\n\nstart_time = time.time()\ntips_df = load_data(path, \"yelp_academic_dataset_tip.json\")\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Tips Load Time: {elapsed_time:.4f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:53:09.703482Z","iopub.execute_input":"2025-02-18T21:53:09.703932Z","iopub.status.idle":"2025-02-18T21:53:19.920365Z","shell.execute_reply.started":"2025-02-18T21:53:09.703891Z","shell.execute_reply":"2025-02-18T21:53:19.919475Z"}},"outputs":[{"name":"stdout","text":"\nDataset Load Times:\n\nBusiness Load Time: 3.8558 seconds\nReview Load Time: 1.2043 seconds\nTips Load Time: 5.1393 seconds\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"\n# Clean Data","metadata":{}},{"cell_type":"markdown","source":"Current Thoughts:\n* Should we be counting categories weighted based on reviews, or should we just count categories by businesses?\n* Do we need to lemmatize for a BERT model considering it has been trained? Also, can lemmatized words be tokenized?","metadata":{}},{"cell_type":"code","source":"#Filter out Data that Belongs to Businesses Under a Certain Threshold\nminimum_business_reviews = 30\nreviews_df = reviews_df[reviews_df['business_id'].map(reviews_df['business_id'].value_counts()).gt(minimum_business_reviews)]\n\n#Create Merged DataFrame of Remaining Reviewed Businesses, and Split Categories into List\ndf_rb = pd.merge(reviews_df, businesses_df, on=\"business_id\")\ndf_rb['categories'] = df_rb['categories'].str.split(\", \")\n\n#Print Metrics\nprint(f\"\\nNumber of Total Businesses Represented: {len(df_rb['business_id'].unique())}\")\nprint(f\"Number of Total Reviews: {len(df_rb)}\")\nprint(f\"Percentage of Reviews Kept: {len(df_rb)/num_rev_load*100}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:53:19.921846Z","iopub.execute_input":"2025-02-18T21:53:19.922108Z","iopub.status.idle":"2025-02-18T21:53:20.297222Z","shell.execute_reply.started":"2025-02-18T21:53:19.922087Z","shell.execute_reply":"2025-02-18T21:53:20.296309Z"}},"outputs":[{"name":"stdout","text":"\nNumber of Total Businesses Represented: 718\nNumber of Total Reviews: 50182\nPercentage of Reviews Kept: 50.182%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Dictionary to Store Categories, and Variables to Report Later\nall_unique_cats = {}\ntotal_cats_before = df_rb['categories'].apply(len).sum()\nlen_before = len(df_rb)\n\n#Iterate through each Business' Categories and Count Occurances (Weighted Based on Reviews)\nfor cats in df_rb['categories']:\n    for cat in cats:\n        if cat in all_unique_cats:\n            all_unique_cats[cat] += 1\n        else:\n            all_unique_cats[cat] = 1\n\n#Number of Top Categories to Keep\nnum_cats = 30\nsorted_cats = sorted(all_unique_cats.items(), key=lambda x: x[1], reverse=True)\n\n#Select Top Categories\ntop_cats = [x[0] for x in sorted_cats[:num_cats]]\n\n#Remove Reviews that Belong to Businesses with None of the Top Categories\ndf_rb = df_rb[df_rb['categories'].apply(lambda x: any(cat in top_cats for cat in x) if x else False)]\n\n#Remove Categories from Remaining Reviews that are not in the Top Categories\ndf_rb['categories'] = df_rb['categories'].apply(lambda x: ([i for i in x if i in top_cats]))\n\n#Count Total Number of Categories\ntotal_cats_after = df_rb['categories'].apply(len).sum()\n\nprint(f\"\\nTotal Initial Unique Categories: {len(all_unique_cats)}\")\nprint(f\"Number of Total Reviews: {len(df_rb)}\")\nprint(f\"Percentage of Reviews Kept: {len(df_rb)/num_rev_load*100}%\")\nprint(f\"Average Number of Categories Before: {total_cats_before/len_before}\")\nprint(f\"Average Number of Categories After: {total_cats_after/len(df_rb)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:53:20.298798Z","iopub.execute_input":"2025-02-18T21:53:20.299133Z","iopub.status.idle":"2025-02-18T21:53:20.678991Z","shell.execute_reply.started":"2025-02-18T21:53:20.299109Z","shell.execute_reply":"2025-02-18T21:53:20.677920Z"}},"outputs":[{"name":"stdout","text":"\nTotal Initial Unique Categories: 274\nNumber of Total Reviews: 49374\nPercentage of Reviews Kept: 49.374%\nAverage Number of Categories Before: 5.821170937786457\nAverage Number of Categories After: 4.18432778385385\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#Clean review text field\nsw = stopwords.words('english')\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-zA-Z?.,!Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    #text = re.sub(r\"http\", \"\",text)\n    html = re.compile(r'<.*?>')\n    text = html.sub(r'',text) #Removing html tags\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^,' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    text = \" \".join(text) #removing stopwords\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    return text\n\ndf_rb['text'] = df_rb['text'].apply(lambda x: clean_text(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:53:22.049858Z","iopub.execute_input":"2025-02-18T21:53:22.050248Z","iopub.status.idle":"2025-02-18T21:53:35.568520Z","shell.execute_reply.started":"2025-02-18T21:53:22.050177Z","shell.execute_reply":"2025-02-18T21:53:35.567321Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load the spaCy English model\nnlp = spacy.load('en_core_web_sm', disable=[\"parser\",\"ner\"])\n\n\ndef lemmatize(text):\n    # Process the text using spaCy\n    start_time = time.time()\n    doc = nlp(text)\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"Step 1 Load Time: {elapsed_time:.4f} seconds\")\n    \n    # Extract lemmatized tokens\n    start_time = time.time()\n    lemmatized_tokens = [token.lemma_ for token in doc]\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"Step 2 Load Time: {elapsed_time:.4f} seconds\")\n    \n    # Join the lemmatized tokens into a sentence\n    lemmatized_text = ' '.join(lemmatized_tokens)\n\n    return lemmatized_text\n\n#Uncomment when running on better computer\n#df_rb['text'] = df_rb['text'].apply(lambda x: lemmatize(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T03:09:17.694704Z","iopub.status.idle":"2025-02-18T03:09:17.695178Z","shell.execute_reply":"2025-02-18T03:09:17.694983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# BERT Model","metadata":{}},{"cell_type":"markdown","source":"References:\n* https://github.com/chrizchow/BERTClassifier/blob/main/BertMultiLabelClassifier.ipynb\n* https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification#Data-preprocessing","metadata":{}},{"cell_type":"code","source":"#Create inputs (review text) and outputs (categories)\ninputs = df_rb.text.values\n\n#One-Hot Encode\nmlb = MultiLabelBinarizer()\noutputs = mlb.fit_transform(df_rb['categories']).astype(float)\n\n#Total Number of Unique Labels\nnum_labels = outputs.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:55:25.606293Z","iopub.execute_input":"2025-02-18T21:55:25.606648Z","iopub.status.idle":"2025-02-18T21:55:25.708115Z","shell.execute_reply.started":"2025-02-18T21:55:25.606622Z","shell.execute_reply":"2025-02-18T21:55:25.707349Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:55:26.160776Z","iopub.execute_input":"2025-02-18T21:55:26.161101Z","iopub.status.idle":"2025-02-18T21:55:29.944357Z","shell.execute_reply.started":"2025-02-18T21:55:26.161076Z","shell.execute_reply":"2025-02-18T21:55:29.943258Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a38c79284c4c4e98f1463f0ceaa211"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ed6829ea2142ab8a51d4b11f154f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8112e62a206498a95918f809e691109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee7476743f54870997b7a9a10163a2a"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"#Make sure Tokenizer is Working\nprint('\\nOriginal: {}\\n'.format(inputs[0]))\n\n# Print the sentence split into tokens.\nprint('Tokenized: {}\\n'.format(tokenizer.tokenize(inputs[0])))\n\n# Print the sentence mapped to token ids.\nprint('Token IDs: {}'.format(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(inputs[0]))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:55:31.453542Z","iopub.execute_input":"2025-02-18T21:55:31.453900Z","iopub.status.idle":"2025-02-18T21:55:31.461648Z","shell.execute_reply.started":"2025-02-18T21:55:31.453871Z","shell.execute_reply":"2025-02-18T21:55:31.460744Z"}},"outputs":[{"name":"stdout","text":"\nOriginal: wow yummy different delicious favorite lamb curry korma different kinds naan let outside deter almost changed minds go try something new glad\n\nTokenized: ['wow', 'yu', '##mmy', 'different', 'delicious', 'favorite', 'lamb', 'curry', 'ko', '##rma', 'different', 'kinds', 'na', '##an', 'let', 'outside', 'deter', 'almost', 'changed', 'minds', 'go', 'try', 'something', 'new', 'glad']\n\nToken IDs: [10166, 9805, 18879, 2367, 12090, 5440, 12559, 15478, 12849, 17830, 2367, 7957, 6583, 2319, 2292, 2648, 28283, 2471, 2904, 9273, 2175, 3046, 2242, 2047, 5580]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\nprint('Start Run')\nstart_time = time.time()\n\n# For every review...\nfor rev in inputs:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        rev,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        padding = 'max_length',           \n                        max_length = 200,           # Pad & truncate all sentences. Reviews over 512 so truncate there for now.\n                        pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                        truncation = True        # Truncate at max_length\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Tokenizer Encoding: {elapsed_time:.4f} seconds\")\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\noutputs = torch.tensor(outputs).clone().detach()\n\n# Print sentence 0, now as a list of IDs.\nprint('Original: ', inputs[0])\nprint('Token IDs:', input_ids[0])\nprint('Done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T21:56:15.086764Z","iopub.execute_input":"2025-02-18T21:56:15.087081Z","iopub.status.idle":"2025-02-18T21:57:45.741064Z","shell.execute_reply.started":"2025-02-18T21:56:15.087057Z","shell.execute_reply":"2025-02-18T21:57:45.740044Z"}},"outputs":[{"name":"stdout","text":"Start Run\nTokenizer Encoding: 90.0699 seconds\nOriginal:  wow yummy different delicious favorite lamb curry korma different kinds naan let outside deter almost changed minds go try something new glad\nToken IDs: tensor([  101, 10166,  9805, 18879,  2367, 12090,  5440, 12559, 15478, 12849,\n        17830,  2367,  7957,  6583,  2319,  2292,  2648, 28283,  2471,  2904,\n         9273,  2175,  3046,  2242,  2047,  5580,   102,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\nDone\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, outputs)\n\n# Create a 75-15-10 train-validation-test split.\n\n# Calculate the number of samples to include in each set.\ntrain_size = int(0.75 * len(dataset))\nval_size_b4_test = int(len(dataset) - train_size)\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size_b4_test])\n\nval_size = int(0.15 * len(dataset))\ntest_size = int(len(val_dataset)-val_size)\nval_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))\nprint('{:>5,} testing samples'.format(test_size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:54:50.515059Z","iopub.execute_input":"2025-02-18T22:54:50.515438Z","iopub.status.idle":"2025-02-18T22:54:50.528608Z","shell.execute_reply.started":"2025-02-18T22:54:50.515410Z","shell.execute_reply":"2025-02-18T22:54:50.527384Z"}},"outputs":[{"name":"stdout","text":"37,030 training samples\n7,406 validation samples\n4,938 testing samples\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# The DataLoader needs to know our batch size for training, so we specify it \n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )\n\ntest_dataloader = DataLoader(\n            test_dataset, # The validation samples.\n            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:54:52.449836Z","iopub.execute_input":"2025-02-18T22:54:52.450166Z","iopub.status.idle":"2025-02-18T22:54:52.455778Z","shell.execute_reply.started":"2025-02-18T22:54:52.450142Z","shell.execute_reply":"2025-02-18T22:54:52.454631Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = num_labels, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n    problem_type = \"multi_label_classification\" # Defaults loss function to BCEWithLogitsLoss\n)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# if device == \"cuda:0\":\n# # Tell pytorch to run this model on the GPU.\n#     model = model.cuda()\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:54:52.706802Z","iopub.execute_input":"2025-02-18T22:54:52.707133Z","iopub.status.idle":"2025-02-18T22:54:53.636043Z","shell.execute_reply.started":"2025-02-18T22:54:52.707107Z","shell.execute_reply":"2025-02-18T22:54:53.635201Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\n# Define Loss function Compatible with Multi-label classification may be redundant givent that \"problem_type\" specified as multi-label-classification in model\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:54:53.637296Z","iopub.execute_input":"2025-02-18T22:54:53.637558Z","iopub.status.idle":"2025-02-18T22:54:53.643299Z","shell.execute_reply.started":"2025-02-18T22:54:53.637535Z","shell.execute_reply":"2025-02-18T22:54:53.642173Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:54:55.575479Z","iopub.execute_input":"2025-02-18T22:54:55.575836Z","iopub.status.idle":"2025-02-18T22:54:55.580412Z","shell.execute_reply.started":"2025-02-18T22:54:55.575809Z","shell.execute_reply":"2025-02-18T22:54:55.579302Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"###########################\n# Train with training set #\n###########################\ndef train(model, iterator, optimizer, criterion, device, scheduler, epoch):\n    \n    # Enter Train Mode\n    model.train()\n    train_loss = 0\n\n    #Number of iterations equal to total train dataset / batch size\n    for step, batch in enumerate(iterator):\n        #Print progress in epoch\n        print(f\"Progress: {step+1}/{len(iterator)}\", end='\\r')\n        # Parse iterator tensor dataset for important information\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        output = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n        \n        # Generate prediction\n        optimizer.zero_grad()\n        \n        # Compute gradients and update weights\n        loss = criterion(output.logits, b_labels) # BCEWithLogitsLoss has sigmoid\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        # accumulate train loss\n        train_loss += loss\n    \n    # print completed result\n    print()\n    print('Train Loss: %f' % (train_loss))\n    return train_loss\n\n#############################\n# Validate with testing set #\n#############################\ndef test(model, iterator, optimizer, criterion, device, epoch):\n\n    # Enter Evaluation Mode\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for step, batch in enumerate(iterator):\n        \n            #Print progress in epoch\n            print(f\"Progress: {step+1}/{len(iterator)}\", end='\\r')\n            \n            # Parse iterator tensor dataset for important information\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            \n            # generate prediction\n            output = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n            prob = output.logits.sigmoid()   # BCEWithLogitsLoss has sigmoid\n            \n            # record processed data count\n            total += (b_labels.size(0)*b_labels.size(1))\n\n            # take the index of the highest prob as prediction output\n            THRESHOLD = 0.7\n            prediction = prob.detach().clone()\n            prediction[prediction > THRESHOLD] = 1\n            prediction[prediction <= THRESHOLD] = 0\n            correct += prediction.eq(b_labels).sum().item()\n\n        print()\n    \n    #print completed result\n    acc = 100.*correct/total\n    print('Correct: %i  / Total: %i / Test Accuracy: %f' % (correct, total, acc))\n    return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:54:56.038128Z","iopub.execute_input":"2025-02-18T22:54:56.038508Z","iopub.status.idle":"2025-02-18T22:54:56.049205Z","shell.execute_reply.started":"2025-02-18T22:54:56.038479Z","shell.execute_reply":"2025-02-18T22:54:56.048279Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"for e in range(epochs):\n    \n    print(f\"\\n===== Epoch {e+1}/{epochs} =====\")\n    \n    # training\n    print(\"Training started ...\")\n    train(model, train_dataloader, optimizer, criterion, device, scheduler, e)\n\n    # validation testing\n    print(\"Testing started ...\")\n    test(model, validation_dataloader, optimizer, criterion, device, e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load best model\nmodel = torch.load('bert_model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Will have to adjust this but not sure how to do so yet without the model - save for later\npredictions = []\nfor batch in test_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        with torch.no_grad():        \n            output= model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask)\n            logits = output.logits\n            logits = logits.detach().cpu().numpy()\n            pred_flat = np.argmax(logits, axis=1).flatten()\n            \n            predictions.extend(list(pred_flat))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}