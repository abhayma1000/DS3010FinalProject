{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import time\n",
    "\n",
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.gmail.com\"  # For Gmail, change if using Outlook or another service\n",
    "SMTP_PORT = 465  # SSL port (or use 587 for TLS)\n",
    "SENDER_EMAIL = \"abhaymathur1000@gmail.com\"  # Replace with your email\n",
    "SENDER_PASSWORD = \"blom axpb quot zcdv\"  # Replace with your email password\n",
    "RECEIVER_EMAIL = \"abhaymathur1000@gmail.com\"  # Your email (or another recipient)\n",
    "\n",
    "def send_email(subject, body=\"\"):\n",
    "    \"\"\"Sends an email notification\"\"\"\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = SENDER_EMAIL\n",
    "    msg[\"To\"] = RECEIVER_EMAIL\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
    "            server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Notify when the script completes\n",
    "# send_email(\"✅ Script Completed\", \"Your Python script has finished running.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def get_time_from_start(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{seconds:05.2f}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/abhay/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/abhay/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/abhay/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abhay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/abhay/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer, get_linear_schedule_with_warmup, BertModel\n",
    "from IPython.display import display, clear_output\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "os.system('./venv/bin/python -m spacy download en_core_web_sm')\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    nltk.download('punkt_tab')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/abhay/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"/kaggle\"):\n",
    "    path = \"/kaggle/input/yelp-dataset\"\n",
    "elif os.path.exists('/home/abhaydesktop/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4'):\n",
    "    path = '/home/abhaydesktop/.cache/kagglehub/datasets/yelp-dataset/yelp-dataset/versions/4'\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"yelp-dataset/yelp-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path, filename, chunk_size=10000):\n",
    "    chunks = pd.read_json(os.path.join(path, filename), lines=True, chunksize=chunk_size)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        df_list.append(chunk)\n",
    "\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\tdef __init__(self, data_path, tensor_path=None, model_name=\"bert-base-uncased\", batch_size=32, epochs=1, n_part=0):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the BertClassifier.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tdata_path (str): Path to the directory containing the Parquet files.\n",
    "\t\t\tmodel_name (str, optional): Name of the BERT model to use. Defaults to \"bert-base-uncased\".\n",
    "\t\t\tnum_labels (int, optional): Number of output labels. If None, it will be inferred from the data. Defaults to None.\n",
    "\t\t\tbatch_size (int, optional): Batch size for training and evaluation. Defaults to 32.\n",
    "\t\t\tepochs (int, optional): Number of training epochs. Defaults to 1.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(BertClassifier, self).__init__()\n",
    "\n",
    "\t\tself.data_path = data_path\n",
    "\t\tself.model_name = model_name\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.tokenizer_max_length = 512\n",
    "\t\tself.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t\tif tensor_path is None:\n",
    "\t\t\tprint(\"Loading in the data...\")\n",
    "\n",
    "\t\t\t# Load data and preprocess\n",
    "\t\t\tif n_part > 0:\n",
    "\t\t\t\tfile_count = len([f for f in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, f))])\n",
    "\n",
    "\t\t\t\tfor i in range(min(n_part, file_count)):\n",
    "\t\t\t\t\tread_df = dd.read_parquet(os.path.join(self.data_path, f\"output_part_{i}.parquet\"), engine=\"pyarrow\")\n",
    "\t\t\t\t\tself.df_dask = read_df if i == 0 else dd.concat([self.df_dask, read_df])\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.df_dask = dd.read_parquet(self.data_path, engine=\"pyarrow\")\n",
    "\t\t\t\n",
    "\t\t\tself.all_df = self.df_dask.compute()\n",
    "\n",
    "\t\t\tprint(\"Loaded in the data\")\n",
    "\t\t\t\n",
    "\t\t\t# Set up the data specific stuff\n",
    "\t\t\tself.businesses_df = load_data(path, \"yelp_academic_dataset_business.json\")\n",
    "\n",
    "\t\t\t# Sample 20 reviews per business and concate\n",
    "\t\t\tdef get_adj(text):\n",
    "\t\t\t\t\ttokenized = pos_tag(word_tokenize(text), tagset='universal')\n",
    "\t\t\t\t\t# print(tokenized)\n",
    "\t\t\t\t\tadjs = [word for word, pos in tokenized if pos == 'ADJ']\n",
    "\t\t\t\t\treturn ' '.join(adjs)\n",
    "\n",
    "\t\t\tdef sentiment_summary(text, fraction=0.3):\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tSummarizes text while preserving sentiment.\n",
    "\t\t\t\t\n",
    "\t\t\t\t:param text: The input text to summarize\n",
    "\t\t\t\t:param fraction: The percentage of original text to keep (0.0 to 1.0)\n",
    "\t\t\t\t:return: A summarized version of the text\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tsentences = sent_tokenize(text)\n",
    "\t\t\t\ttotal_sentences = len(sentences)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Ensure at least 1 sentence is returned\n",
    "\t\t\t\tnum_sentences = max(1, int(total_sentences * fraction))\n",
    "\n",
    "\t\t\t\tvectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\t\t\t\ttfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\t\t\t\t\n",
    "\t\t\t\tsentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Get sentiment scores\n",
    "\t\t\t\tanalyzer = SentimentIntensityAnalyzer()\n",
    "\t\t\t\tsentiment_scores = [abs(analyzer.polarity_scores(s)['compound']) for s in sentences]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Combine importance & sentiment (weighted sum)\n",
    "\t\t\t\tcombined_scores = sentence_scores * np.array(sentiment_scores)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Pick top sentences\n",
    "\t\t\t\ttop_sentence_indices = combined_scores.argsort()[-num_sentences:][::-1]\n",
    "\t\t\t\tsummary = [sentences[i] for i in sorted(top_sentence_indices)]\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn \" \".join(summary)\n",
    "\t\t\t\n",
    "\t\t\t# Print the length of all the texts\n",
    "\t\t\tprint(\"Length of all the texts before anything: \", self.all_df['text'].apply(lambda x: len(x)).describe())\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfrac = 0.3\n",
    "\t\t\t\tself.all_df['text'] = self.all_df['text'].apply(lambda x : sentiment_summary(x, fraction=frac))\n",
    "\n",
    "\t\t\t\t# Print the length of all the texts\n",
    "\t\t\t\tprint(\"Length of all the texts after sentiment summary: \", self.all_df['text'].apply(lambda x: len(x)).describe())\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(\"Error in sentiment summary, skipping over\")\n",
    "\n",
    "\n",
    "\t\t\tself.all_df['text'] = self.all_df['text'].apply(lambda x : get_adj(x))\n",
    "\n",
    "\t\t\t# Print the length of all the texts\n",
    "\t\t\tprint(\"Length of all the texts after adj only: \", self.all_df['text'].apply(lambda x: len(x)).describe())\n",
    "\n",
    "\t\t\t# Cap the text at a certain length\n",
    "\t\t\tself.all_df['text'] = self.all_df['text'].apply(lambda x: x[:int(self.tokenizer_max_length / 5)])\n",
    "\n",
    "\t\t\t# Print the length of all the texts\n",
    "\t\t\tprint(\"Length of all the texts after capping: \", self.all_df['text'].apply(lambda x: len(x)).describe())\n",
    " \n",
    "\t\t\tdef calculate_return_guest_percentage(reviews_business_df):\n",
    "\t\t\t\tdf = reviews_business_df  # Ensure it has columns: 'user_id', 'business_id', 'text'\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Count total unique guests per business\n",
    "\t\t\t\ttotal_guests = df.groupby('business_id')['user_id'].nunique().reset_index()\n",
    "\t\t\t\ttotal_guests = total_guests.rename(columns={'user_id': 'total_guest_count'})\n",
    "\n",
    "\t\t\t\t# Count the number of reviews per user per business\n",
    "\t\t\t\trepeat_visits = df.groupby(['business_id', 'user_id']).size().reset_index(name='review_count')\n",
    "\n",
    "\t\t\t\t# Filter businesses where a user has more than one review (return guests)\n",
    "\t\t\t\trepeat_visits = repeat_visits[repeat_visits['review_count'] > 1]\n",
    "\n",
    "\t\t\t\t# Count return guests per business\n",
    "\t\t\t\treturn_guests = repeat_visits.groupby('business_id')['user_id'].nunique().reset_index()\n",
    "\t\t\t\treturn_guests = return_guests.rename(columns={'user_id': 'return_guest_count'})\n",
    "\n",
    "\t\t\t\t# Merge return guests count with total guests count\n",
    "\t\t\t\tresult = total_guests.merge(return_guests, on='business_id', how='left').fillna(0)\n",
    "\n",
    "\t\t\t\t# Calculate return guest percentage\n",
    "\t\t\t\tresult['return_percentage'] = (result['return_guest_count'] / result['total_guest_count']) * 100\n",
    "\t\t\t\tresult['return_percentage'] = result['return_percentage'].fillna(0)  # Replace NaN with 0%\n",
    "\n",
    "\t\t\t\treturn result\n",
    "\t\t\t\n",
    "\t\t\treturn_guest_percentage = calculate_return_guest_percentage(self.all_df)\n",
    "\n",
    "\n",
    "\t\t\tsampled_reviews = self.all_df.groupby('business_id')['text'].apply(lambda x: ' '.join(x.sample(min(len(x), 20))))\n",
    "\t\t\tsampled_reviews_businesses_df = pd.merge(self.businesses_df, sampled_reviews, on='business_id', how='inner')\n",
    "\t\t\tself.all_df['date'] = pd.to_datetime(self.all_df['date'])\n",
    "\n",
    "\t\t\tlast_review_dates = self.all_df.groupby('business_id')['date'].max().reset_index()\n",
    "\n",
    "\t\t\tbusinesses_before_2020 = last_review_dates[last_review_dates['date'] < '2020-01-01']\n",
    "\n",
    "\t\t\tsampled_reviews_businesses_not_after_2020 = pd.merge(sampled_reviews_businesses_df, businesses_before_2020, on='business_id', how='inner')\n",
    "\t\t\tsampled_reviews_businesses_after_2020 = sampled_reviews_businesses_df[~sampled_reviews_businesses_df['business_id'].isin(businesses_before_2020['business_id'])]\n",
    "\n",
    "\t\t\t# add return_guest_percentage to all_df\n",
    "\t\t\tsampled_reviews_businesses_not_after_2020 = sampled_reviews_businesses_not_after_2020.merge(return_guest_percentage, on='business_id', how='left')\n",
    "\t\t\tsampled_reviews_businesses_after_2020 = sampled_reviews_businesses_after_2020.merge(return_guest_percentage, on='business_id', how='left')\n",
    "\n",
    "\t\t\tprint(\"Return Guest Percentage: \", sampled_reviews_businesses_not_after_2020['return_percentage'].describe())\n",
    "\n",
    "\t\t\tdef return_useful_features(df):\n",
    "\t\t\t\tname_counts = df['name'].value_counts()\n",
    "\t\t\t\tdf['franchise_score'] = df['name'].apply(lambda x: np.log(name_counts[x]))\n",
    "\t\t\t\tdf['franchise_score'] = df['franchise_score'].fillna(0).astype(float)\n",
    "\n",
    "\t\t\t\t# One hot encode the state and add it to df\n",
    "\t\t\t\tdf = pd.concat([df, pd.get_dummies(df['state'])], axis=1)\n",
    "\t\t\t\tprint(\"DF columns: \", df.columns)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t\tcoords = df[['latitude', 'longitude']].values.T\n",
    "\n",
    "\t\t\t\tkde = gaussian_kde(coords)\n",
    "\t\t\t\tdf['density_score'] = kde(coords).astype(float)\n",
    "\n",
    "\t\t\t\t# Normalize it\n",
    "\t\t\t\tdf['density_score'] = (df['density_score'] - df['density_score'].min()) / (df['density_score'].max() - df['density_score'].min())\n",
    "\n",
    "\n",
    "\t\t\t\tuseful_features = ['latitude', 'longitude', 'stars', 'review_count', 'franchise_score', 'density_score', 'return_percentage', 'text']\n",
    "\t\t\t\tdf_useful_features = df[useful_features]\n",
    "\t\t\t\treturn df_useful_features, df_useful_features.shape[1] - 1, useful_features\n",
    "\n",
    "\t\t\tsampled_reviews_businesses_not_after_2020, nn_input_size, useful_features = return_useful_features(sampled_reviews_businesses_not_after_2020)\n",
    "\t\t\tself.nn_input_size = nn_input_size\n",
    "\t\t\tsampled_reviews_businesses_after_2020, _, _ = return_useful_features(sampled_reviews_businesses_after_2020)\n",
    "\n",
    "\t\t\tnum_businesses_before_2020 = sampled_reviews_businesses_not_after_2020.shape[0]\n",
    "\t\t\tnum_businesses_after_2020 = sampled_reviews_businesses_after_2020.shape[0]\n",
    "\t\t\tprint(\"Number of businesses before 2020: \", num_businesses_before_2020)\n",
    "\t\t\tprint(\"Number of businesses after 2020: \", num_businesses_after_2020)\n",
    "\t\t\tprint(\"Percentage of businesses before 2020: \", num_businesses_before_2020 / (num_businesses_before_2020 + num_businesses_after_2020))\n",
    "\t\t\tprint(\"Percentage of businesses after 2020: \", num_businesses_after_2020 / (num_businesses_before_2020 + num_businesses_after_2020))\n",
    "\n",
    "\t\t\tsampled_reviews_businesses_not_after_2020['output'] = 0\n",
    "\t\t\tsampled_reviews_businesses_after_2020['output'] = 1\n",
    "\n",
    "\t\t\t# I am taking a stratified sample of the data to better represent the 0s\n",
    "\t\t\tnum_samples = min(num_businesses_before_2020, num_businesses_after_2020)\n",
    "\n",
    "\t\t\tsampled_reviews_businesses_not_after_2020 = sampled_reviews_businesses_not_after_2020.sample(n=num_samples, random_state=42)\n",
    "\t\t\tsampled_reviews_businesses_after_2020 = sampled_reviews_businesses_after_2020.sample(n=num_samples, random_state=42)\n",
    "\n",
    "\n",
    "\t\t\tself.final_dataset = pd.concat([sampled_reviews_businesses_not_after_2020, sampled_reviews_businesses_after_2020], ignore_index=True)\n",
    "\t\t\tprint(\"Total dataset length: \", len(self.final_dataset))\n",
    "\n",
    "\n",
    "\t\tself.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=True)\n",
    "\n",
    "\t\tif tensor_path is not None and os.path.exists(tensor_path):\n",
    "\t\t\ttensors = torch.load(tensor_path)\n",
    "\t\t\tself.bert_inputs = tensors[\"bert_inputs\"]\n",
    "\t\t\tself.nn_inputs = tensors[\"nn_inputs\"]\n",
    "\t\t\tself.nn_input_size = self.nn_inputs.shape[1]\n",
    "\t\t\tself.attention_masks = tensors[\"attention_masks\"]\n",
    "\t\t\tself.outputs = tensors[\"outputs\"]\n",
    "\t\telse:\n",
    "\t\t\tself.bert_inputs, self.nn_inputs, self.attention_masks, self.outputs = self._preprocess_data(useful_features, tensor_save_path=f'data/tensors/tensors_parts{n_part}.pth')\n",
    "\t\tself.train_dataloader, self.validation_dataloader, self.test_dataloader = self._create_dataloaders()\n",
    "\n",
    "\t\tself.model = BertModel.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "\t\tfor param in self.model.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\n",
    "\t\tfor param in self.model.encoder.layer[-2:].parameters():\n",
    "\t\t\tparam.requires_grad = True\n",
    "\n",
    "\t\tself.bert_output_size = self.model.config.hidden_size\n",
    "\n",
    "\t\tself.nn_hidden_size = 64\n",
    "\t\tself.nn_output_size = 32\n",
    "\t\tself.final_hidden_size = 16\n",
    "\n",
    "\t\tself.nn_layers = nn.Sequential(\n",
    "\t\t\tnn.Linear(self.nn_input_size, self.nn_hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(self.nn_hidden_size, self.nn_output_size),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t).to(self.device)\n",
    "\n",
    "\t\tself.final_layers = nn.Sequential(\n",
    "\t\t\tnn.Linear(self.bert_output_size + self.nn_output_size, self.final_hidden_size),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(self.final_hidden_size, 1)\n",
    "\t\t).to(self.device)\n",
    "\n",
    "\t\tself.optimizer = AdamW(self.model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\t\tself.scheduler = self._get_scheduler()\n",
    "\t\tself.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\tdef _preprocess_data(self, useful_features, tensor_save_path=None):\n",
    "\t\t\"\"\"Preprocesses the data.\"\"\"\n",
    "\t\tinputs = self.final_dataset[\"text\"].values\n",
    "\t\tprint(\"Inputs: \", inputs)\n",
    "\t\tprint(\"Inputs Shape: \", inputs.shape)\n",
    "\t\tprint(\"Dataset Useful Features: \", self.final_dataset[useful_features].head(5))\n",
    "\n",
    "\t\tif \"text\" in useful_features:\n",
    "\t\t\tuseful_features.remove(\"text\")\n",
    "\n",
    "\t\t# Ensure all useful features are numeric and handle missing values\n",
    "\t\tfor feature in useful_features:\n",
    "\t\t\tself.final_dataset[feature] = pd.to_numeric(self.final_dataset[feature], errors='coerce').fillna(0)\n",
    "\n",
    "\t\t# Convert to torch tensor\n",
    "\t\tnn_inputs = torch.tensor(self.final_dataset[useful_features].values, dtype=torch.float32)\n",
    "\t\t\n",
    "\t\tprint(\"NN Inputs: \", nn_inputs)\n",
    "\t\tprint(\"NN Inputs Shape: \", nn_inputs.shape)\n",
    "\n",
    "\t\toutputs = self.final_dataset[\"output\"].values\n",
    "\t\tprint(\"Outputs: \", outputs)\n",
    "\n",
    "\t\tinput_ids = []\n",
    "\t\tattention_masks = []\n",
    "\n",
    "\t\tprint('Start Tokenization')\n",
    "\t\tstart_time = time.time()\n",
    "\n",
    "\t\t# For every review...\n",
    "\t\tfor rev in inputs:\n",
    "\t\t\t# `encode_plus` will:\n",
    "\t\t\t#   (1) Tokenize the sentence.\n",
    "\t\t\t#   (2) Prepend the `[CLS]` token to the start.\n",
    "\t\t\t#   (3) Append the `[SEP]` token to the end.\n",
    "\t\t\t#   (4) Map tokens to their IDs.\n",
    "\t\t\t#   (5) Pad or truncate the sentence to `max_length`\n",
    "\t\t\t#   (6) Create attention masks for [PAD] tokens.\n",
    "\t\t\tencoded_dict = self.tokenizer.encode_plus(\n",
    "\t\t\t\trev,  # Sentence to encode.\n",
    "\t\t\t\tadd_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\tmax_length=self.tokenizer_max_length,  # Pad & truncate all sentences. Reviews over 512 so truncate there for now.\n",
    "\t\t\t\tpad_to_max_length=True,\n",
    "\t\t\t\treturn_attention_mask=True,  # Construct attn. masks.\n",
    "\t\t\t\treturn_tensors='pt',  # Return pytorch tensors.\n",
    "\t\t\t\ttruncation=True  # Truncate at max_length\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Add the encoded sentence to the list.\n",
    "\t\t\tinput_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "\t\t\t# And its attention mask (simply differentiates padding from non-padding).\n",
    "\t\t\tattention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\t\tend_time = time.time()\n",
    "\t\telapsed_time = end_time - start_time\n",
    "\t\tprint(f\"Tokenizer Encoding: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "\t\t# Convert the lists into tensors.\n",
    "\t\tinput_ids = torch.cat(input_ids, dim=0)\n",
    "\t\tattention_masks = torch.cat(attention_masks, dim=0)\n",
    "\t\toutputs = torch.tensor(outputs).clone().detach()\n",
    "\n",
    "\t\tif tensor_save_path is not None:\n",
    "\t\t\ttorch.save({\"bert_inputs\": input_ids, \"nn_inputs\": nn_inputs, \"attention_masks\": attention_masks, \"outputs\": outputs}, tensor_save_path)\n",
    "\n",
    "\n",
    "\t\treturn input_ids, nn_inputs, attention_masks, outputs\n",
    "\n",
    "\tdef _create_dataloaders(self):\n",
    "\t\t\"\"\"Creates dataloaders for training, validation, and testing.\"\"\"\n",
    "\t\t# Combine the training inputs into a TensorDataset.\n",
    "\t\tprint(\"Bert input: \", self.bert_inputs.shape)\n",
    "\t\tprint(\"NN input: \", self.nn_inputs.shape)\n",
    "\t\tprint(\"Attention Mask: \", self.attention_masks.shape)\n",
    "\t\tprint(\"Outputs: \", self.outputs.shape)\n",
    "\t\tdataset = TensorDataset(self.bert_inputs, self.nn_inputs, self.attention_masks, self.outputs)\n",
    "\n",
    "\t\t# Create a 75-15-10 train-validation-test split.\n",
    "\n",
    "\t\t# Calculate the number of samples to include in each set.\n",
    "\t\ttrain_size = int(0.75 * len(dataset))\n",
    "\t\tval_size_b4_test = int(len(dataset) - train_size)\n",
    "\n",
    "\t\t# Divide the dataset by randomly selecting samples.\n",
    "\t\ttrain_dataset, val_dataset = random_split(dataset, [train_size, val_size_b4_test])\n",
    "\n",
    "\t\tval_size = int(0.15 * len(dataset))\n",
    "\t\ttest_size = int(len(val_dataset) - val_size)\n",
    "\t\tval_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n",
    "\n",
    "\t\tprint('{:>5,} training samples'.format(train_size))\n",
    "\t\tprint('{:>5,} validation samples'.format(val_size))\n",
    "\t\tprint('{:>5,} testing samples'.format(test_size))\n",
    "\n",
    "\t\t# Create the DataLoaders for our training and validation sets.\n",
    "\t\t# We'll take training samples in random order.\n",
    "\t\ttrain_dataloader = DataLoader(\n",
    "\t\t\ttrain_dataset,  # The training samples.\n",
    "\t\t\tsampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "\t\t\tbatch_size=self.batch_size  # Trains with this batch size.\n",
    "\t\t)\n",
    "\n",
    "\t\t# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "\t\tvalidation_dataloader = DataLoader(\n",
    "\t\t\tval_dataset,  # The validation samples.\n",
    "\t\t\tsampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "\t\t\tbatch_size=self.batch_size  # Evaluate with this batch size.\n",
    "\t\t)\n",
    "\n",
    "\t\ttest_dataloader = DataLoader(\n",
    "\t\t\ttest_dataset,  # The validation samples.\n",
    "\t\t\tsampler=SequentialSampler(test_dataset),  # Pull out batches sequentially.\n",
    "\t\t\tbatch_size=self.batch_size  # Evaluate with this batch size.\n",
    "\t\t)\n",
    "\n",
    "\t\treturn train_dataloader, validation_dataloader, test_dataloader\n",
    "\n",
    "\tdef _get_scheduler(self):\n",
    "\t\t\"\"\"Creates the learning rate scheduler.\"\"\"\n",
    "\t\ttotal_steps = len(self.train_dataloader) * self.epochs\n",
    "\t\tscheduler = get_linear_schedule_with_warmup(self.optimizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tnum_warmup_steps=0,  # Default value in run_glue.py\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tnum_training_steps=total_steps)\n",
    "\t\treturn scheduler\n",
    "\n",
    "\n",
    "\tdef forward(self, input_ids, attention_mask, nn_input):\n",
    "\t\tbert_outputs = self.model(input_ids, attention_mask)\n",
    "\t\tpooled_output = bert_outputs.pooler_output\n",
    "\n",
    "\t\tnn_outputs = self.nn_layers(nn_input)\n",
    "\t\tfinal_input = torch.cat((pooled_output, nn_outputs), dim=1)\n",
    "\t\tfinal_output = self.final_layers(final_input)\n",
    "\n",
    "\t\treturn final_output\n",
    "\n",
    "\tdef train_model(self):\n",
    "\t\tself.train_losses = []\n",
    "\t\tself.val_losses = []\n",
    "\t\tself.train_accuracies = []\n",
    "\t\tself.val_accuracies = []\n",
    "\t\t\"\"\"Trains the model.\"\"\"\n",
    "\t\tfor e in range(self.epochs):\n",
    "\t\t\tprint(f\"\\n===== Epoch {e + 1}/{self.epochs} =====\")\n",
    "\n",
    "\t\t\t# training\n",
    "\t\t\tprint(\"Training started ...\")\n",
    "\t\t\tself._train_epoch(e)\n",
    "\n",
    "\t\t\t# validation testing\n",
    "\t\t\tprint(\"Testing started ...\")\n",
    "\t\t\tself._test_epoch(self.validation_dataloader, e)\n",
    "\n",
    "\tdef _train_epoch(self, epoch):\n",
    "\t\t\"\"\"Trains the model for one epoch.\"\"\"\n",
    "\t\tself.train()\n",
    "\t\ttrain_loss = 0\n",
    "\n",
    "\n",
    "\t\t# Number of iterations equal to total train dataset / batch size\n",
    "\t\tfor step, batch in enumerate(self.train_dataloader):\n",
    "\t\t\t# Print progress in epoch\n",
    "\t\t\tprint(f\"Progress: {step + 1}/{len(self.train_dataloader)}\", end='\\r')\n",
    "\t\t\t# Parse iterator tensor dataset for important information\n",
    "\t\t\tinput_ids, nn_input, attention_mask, labels = [b.to(self.device) for b in batch]\n",
    "\n",
    "\t\t\t# print(\"Loaded in batch\", end='\\r')\n",
    "\n",
    "\t\t\toutput = self.forward(input_ids, attention_mask, nn_input)\n",
    "\t\t\t\n",
    "\t\t\t# print(\"Fed forward\", end='\\r')\n",
    "\t\t\tlabels = labels.float().view(-1, 1)\n",
    "\t\t\tloss = self.criterion(output, labels)\n",
    "\t\t\t# print(\"Calculate loss\", end='\\r')\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tself.optimizer.step()\n",
    "\t\t\tself.scheduler.step()\n",
    "\n",
    "\t\t\t# print(\"Back Prop\", end='\\r')\n",
    "\n",
    "\n",
    "\t\t\t# accumulate train loss\n",
    "\t\t\ttrain_loss += loss\n",
    "\n",
    "\t\t# print completed result\n",
    "\t\tprint()\n",
    "\t\tprint('Train Loss: %f' % (train_loss))\n",
    "\t\tself.train_losses.append(float(train_loss))\n",
    "\t\treturn train_loss\n",
    "\n",
    "\tdef test_model(self):\n",
    "\t\t\"\"\"Evaluates the model on the test set. Returns accuracy.\"\"\"\n",
    "\t\tself.val_accuracies = []\n",
    "\t\t\n",
    "\t\treturn self._test_epoch(self.test_dataloader, self.epochs - 1)\n",
    "\n",
    "\tdef _test_epoch(self, iterator, epoch):\n",
    "\t\t\"\"\"Evaluates the model on the given dataloader.\"\"\"\n",
    "\t\tself.eval()\n",
    "\t\tcorrect = 0\n",
    "\t\ttotal = 0\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor step, batch in enumerate(iterator):\n",
    "\t\t\t\t# Print progress in epoch\n",
    "\t\t\t\tprint(f\"Progress: {step + 1}/{len(iterator)}\", end='\\r')\n",
    "\t\t\t\t# Parse iterator tensor dataset for important information\n",
    "\t\t\t\tinput_ids, nn_input, attention_mask, labels = [b.to(self.device) for b in batch]\n",
    "\n",
    "\t\t\t\t# generate prediction\n",
    "\t\t\t\toutput = self.forward(input_ids, attention_mask, nn_input)\n",
    "\t\t\t\t\n",
    "\t\t\t\tprob = output.sigmoid()  # BCEWithLogitsLoss has sigmoid\n",
    "\n",
    "\t\t\t\t# print(\"Prob: \", prob)\n",
    "\t\t\t\t# print(\"Prob Shape: \", prob.shape)\n",
    "\n",
    "\t\t\t\t# record processed data count\n",
    "\t\t\t\ttotal += labels.size(0)\n",
    "\n",
    "\t\t\t\t# take the index of the highest prob as prediction output\n",
    "\t\t\t\tprediction = prob.detach().clone()\n",
    "\t\t\t\tprint(\"Original prediction: \", prediction)\n",
    "\t\t\t\tTHRESHOLD = 0.5\n",
    "\t\t\t\tprediction[prediction > THRESHOLD] = 1\n",
    "\t\t\t\tprediction[prediction <= THRESHOLD] = 0\n",
    "\t\t\t\t# print(\"Prediction: \", prediction)\n",
    "\t\t\t\t# print(\"Prediction Shape: \", prediction.shape)\n",
    "\t\t\t\tlabels = labels.float().view(-1, 1)\n",
    "\t\t\t\tprint(\"Prediction and Labels: \", torch.cat((prediction, labels), dim=1))\n",
    "\t\t\t\t# print(\"Labels: \", labels)\n",
    "\t\t\t\t# print(\"Labels Shape: \", labels.shape)\n",
    "\t\t\t\t# print(\"Correct: \", prediction.eq(labels).sum().item())\n",
    "\t\t\t\t# print(\"Incorrect: \", prediction.ne(labels).sum().item())\n",
    "\t\t\t\t\n",
    "\t\t\t\tcorrect += prediction.eq(labels).sum().item()\n",
    "\n",
    "\t\t\t\t# print()\n",
    "\n",
    "\t\t# print completed result\n",
    "\t\tacc = 100. * correct / total\n",
    "\t\tprint('Correct: %i   / Total: %i / Test Accuracy: %f' % (correct, total, acc))\n",
    "\t\tself.val_accuracies.append(float(acc))\n",
    "\t\treturn acc\n",
    "\t\n",
    "\tdef predict(self, text, nn_input):\n",
    "\t\t\"\"\"Predicts the class of the input text.\"\"\"\n",
    "\t\tself.eval()\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\t# Tokenize the input text\n",
    "\t\tinputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.tokenizer_max_length, padding=\"max_length\", truncation=True)\n",
    "\t\tinput_ids = inputs[\"input_ids\"].to(self.device)\n",
    "\t\tattention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "\t\tnn_input = torch.tensor(nn_input, dtype=torch.float32).to(self.device)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutput = self.forward(input_ids, attention_mask, nn_input)\n",
    "\t\t\tprob = output.sigmoid()\n",
    "\t\t\t\n",
    "\t\treturn prob.cpu().numpy()\n",
    "\n",
    "\tdef save(self, save_path):\n",
    "\t\t\"\"\"Saves the trained model.\"\"\"\n",
    "\t\tcount = 0\n",
    "\t\twhile os.path.exists(save_path + str(count)):\n",
    "\t\t\tcount += 1\n",
    "\n",
    "\t\tself.model.save_pretrained(save_path + str(count))\n",
    "\n",
    "\t\tprint(f\"Model saved to {save_path + str(count)}\")\n",
    "\t\n",
    "\t\t# Saving the training stuff\n",
    "\t\tdata = {\n",
    "\t\t\t\"train_loss\": self.train_losses,\n",
    "\t\t\t\"vall_accuracy\": self.val_accuracies,\n",
    "\t\t\t\"epochs\": int(self.epochs),\n",
    "\t\t}\n",
    "\n",
    "\t\t# Define the file path where you want to save the JSON file\n",
    "\t\tfile_path = os.path.join('models', f\"training_data_{count}.json\")\n",
    "\n",
    "\t\t# Save the dictionary as a JSON file\n",
    "\t\twith open(file_path, \"w\") as json_file:\n",
    "\t\t\tjson.dump(data, json_file, indent=4)\n",
    "\n",
    "\t\tprint(f\"Training data saved to {file_path}\")\n",
    "\n",
    "\t\treturn save_path + str(count)\n",
    "\t\n",
    "\tdef load(self, load_path):\n",
    "\t\t\"\"\"Loads a trained model.\"\"\"\n",
    "\t\tself.model = BertModel.from_pretrained(load_path).to(self.device)\n",
    "\n",
    "\t\treturn self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tensor():\n",
    "    data_path = 'data/lemm_stop_reviews_parquet'\n",
    "    tensor_path = None\n",
    "    # Can add tensor_path here to load in tensors from a file\n",
    "    epochs=1\n",
    "    n_part=1\n",
    "    classifier = BertClassifier(data_path, tensor_path=tensor_path, n_part=n_part, epochs=epochs)\n",
    "    # classifier.train_model()\n",
    "    # model_name = classifier.save(os.path.join('models', 'berthybridtrained'))\n",
    "\n",
    "    # send_email(\"✅ BERT Training complete and model saved\", f\"Model took {get_time_from_start(start_time)}\\nEpochs: {epochs}\\nNumber of parts: {n_part}\\nModel name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model():\n",
    "    data_path = 'data/lemm_stop_reviews_parquet'\n",
    "    # tensor_path = 'data/tensors/tensors.pth'\n",
    "    # Can add tensor_path here to load in tensors from a file\n",
    "    # For the main training, using epochs=500\n",
    "    epochs=1\n",
    "    n_part=1\n",
    "    classifier = BertClassifier(data_path, tensor_path=None, n_part=n_part, epochs=epochs)\n",
    "    classifier.train_model()\n",
    "    model_name = classifier.save(os.path.join('models', 'berthybridtrained'))\n",
    "\n",
    "    send_email(\"✅ BERT Training complete and model saved\", f\"Model took {get_time_from_start(start_time)}\\nEpochs: {epochs}\\nNumber of parts: {n_part}\\nModel name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model():\n",
    "    data_path = 'data/lemm_stop_reviews_parquet'\n",
    "    tensor_path = 'data/tensors/tensors_parts1.pth'\n",
    "    model_name = 'berthybridtrained5'\n",
    "    # Can add tensor_path here to load in tensors from a file\n",
    "    epochs=1\n",
    "    n_part=1\n",
    "    classifier = BertClassifier(data_path, tensor_path=tensor_path, n_part=n_part, epochs=epochs)\n",
    "    classifier = classifier.load(os.path.join('models', model_name))\n",
    "    print(f\"Accuracy: {classifier.test_model()}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert input:  torch.Size([324, 512])\n",
      "NN input:  torch.Size([324, 7])\n",
      "Attention Mask:  torch.Size([324, 512])\n",
      "Outputs:  torch.Size([324])\n",
      "  243 training samples\n",
      "   48 validation samples\n",
      "   33 testing samples\n",
      "Original prediction:  tensor([[0.4750],\n",
      "        [0.4533],\n",
      "        [0.5695],\n",
      "        [0.4691],\n",
      "        [0.4940],\n",
      "        [0.5113],\n",
      "        [0.4711],\n",
      "        [0.4770],\n",
      "        [0.5027],\n",
      "        [0.4955],\n",
      "        [0.5196],\n",
      "        [0.4699],\n",
      "        [0.5087],\n",
      "        [0.5250],\n",
      "        [0.4774],\n",
      "        [0.4795],\n",
      "        [0.4808],\n",
      "        [0.4767],\n",
      "        [0.4798],\n",
      "        [0.4897],\n",
      "        [0.4476],\n",
      "        [0.5208],\n",
      "        [0.4863],\n",
      "        [0.4490],\n",
      "        [0.4738],\n",
      "        [0.6003],\n",
      "        [0.4860],\n",
      "        [0.4858],\n",
      "        [0.4798],\n",
      "        [0.4895],\n",
      "        [0.4602],\n",
      "        [0.4907]], device='cuda:0')\n",
      "Prediction and Labels:  tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 1.]], device='cuda:0')\n",
      "Original prediction:  tensor([[0.5201]], device='cuda:0')\n",
      "Prediction and Labels:  tensor([[1., 1.]], device='cuda:0')\n",
      "Correct: 16   / Total: 33 / Test Accuracy: 48.484848\n",
      "Accuracy: 48.484848484848484\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # train_model()\n",
    "    test_model()\n",
    "except Exception as e:\n",
    "    traceback_string = \"\".join(traceback.format_exception(type(e), value=e, tb=sys.exc_info()[2]))\n",
    "    send_email(\"❌ BERT Training failed\", f\"Model took {get_time_from_start(start_time)}\\nError: {e}\\nError stack trace: {traceback_string}\")\n",
    "    print(f\"Error: {e}\\nError stack trace: {traceback_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
