{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:46:54.366485: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-27 23:46:54.390857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740718014.412809  508075 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740718014.418092  508075 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 23:46:54.442881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/abhay/Documents/Repo/abhayma1000/DS3010FinalProject/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "#NLP Packages\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_labels, train_dataloader):\n",
    "        self.num_labels = num_labels\n",
    "        self.train_dataloader = train_dataloader\n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = num_labels, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "            problem_type = \"multi_label_classification\" # Defaults loss function to BCEWithLogitsLoss\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Device: \", device)\n",
    "\n",
    "        # if device == \"cuda:0\":\n",
    "        # # Tell pytorch to run this model on the GPU.\n",
    "        #     model = model.cuda()\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "        # Define Loss function Compatible with Multi-label classification may be redundant givent that \"problem_type\" specified as multi-label-classification in model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "        # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "        # training data.\n",
    "        epochs = 1\n",
    "\n",
    "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "        # (Note that this is not the same as the number of training samples).\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "        # Create the learning rate scheduler.\n",
    "        self.scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "    \n",
    "    ###########################\n",
    "# Train with training set #\n",
    "###########################\n",
    "def train(self, iterator, optimizer, criterion, device, scheduler, epoch):\n",
    "    \n",
    "    # Enter Train Mode\n",
    "    self.model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    #Number of iterations equal to total train dataset / batch size\n",
    "    for step, batch in enumerate(iterator):\n",
    "        #Print progress in epoch\n",
    "        print(f\"Progress: {step+1}/{len(iterator)}\", end='\\r')\n",
    "        # Parse iterator tensor dataset for important information\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        \n",
    "        # Generate prediction\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients and update weights\n",
    "        loss = criterion(output.logits, b_labels) # BCEWithLogitsLoss has sigmoid\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # accumulate train loss\n",
    "        train_loss += loss\n",
    "    \n",
    "    # print completed result\n",
    "    print()\n",
    "    print('Train Loss: %f' % (train_loss))\n",
    "    return train_loss\n",
    "\n",
    "#############################\n",
    "# Validate with testing set #\n",
    "#############################\n",
    "def test(model, iterator, optimizer, criterion, device, epoch):\n",
    "\n",
    "    # Enter Evaluation Mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(iterator):\n",
    "        \n",
    "            #Print progress in epoch\n",
    "            print(f\"Progress: {step+1}/{len(iterator)}\", end='\\r')\n",
    "            print(f'Time per iteration: {datetime.timedelta(seconds=(time.time() - start_time)//(step+1))}', end='\\r')\n",
    "            \n",
    "            # Parse iterator tensor dataset for important information\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            \n",
    "            # generate prediction\n",
    "            output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "            prob = output.logits.sigmoid()   # BCEWithLogitsLoss has sigmoid\n",
    "            \n",
    "            # record processed data count\n",
    "            total += (b_labels.size(0)*b_labels.size(1))\n",
    "\n",
    "            # take the index of the highest prob as prediction output\n",
    "            THRESHOLD = 0.7\n",
    "            prediction = prob.detach().clone()\n",
    "            prediction[prediction > THRESHOLD] = 1\n",
    "            prediction[prediction <= THRESHOLD] = 0\n",
    "            correct += prediction.eq(b_labels).sum().item()\n",
    "\n",
    "        print()\n",
    "    \n",
    "    #print completed result\n",
    "    acc = 100.*correct/total\n",
    "    print('Correct: %i  / Total: %i / Test Accuracy: %f' % (correct, total, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
